{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db48063f-59c2-415e-8651-1a22b4e8c47d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb456947-54f1-4da3-9c93-0404ec94eb43",
   "metadata": {},
   "outputs": [],
   "source": [
    "import arsenal.profiling\n",
    "from collections import deque\n",
    "from arsenal import timers\n",
    "\n",
    "from transduction.lazy import Lazy\n",
    "from transduction.fsa import FSA, EPSILON\n",
    "from transduction import (\n",
    "    FST, EPSILON, PrecoverDecomp, examples, Precover,    \n",
    ")\n",
    "from time import time\n",
    "from collections import Counter\n",
    "from transduction.util import display_table\n",
    "from transduction.precover_nfa import PrecoverNFA as LazyPrecoverNFA\n",
    "from transduction.dfa_decomp_nonrecursive import NonrecursiveDFADecomp\n",
    "from transduction.applications.bpe import Token, bpe_wfst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1141185-96ce-4bcb-9e4c-3f60f5bc9b1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transduction.lm.statelm import decode_hf_tokenizer\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d548702-ptb",
   "metadata": {},
   "source": [
    "# PTB Tokenizer Scaling Benchmark\n",
    "\n",
    "Compares decomposition and LM algorithms on the Penn Treebank tokenizer FST\n",
    "(~296 states, 257 input symbols).  All calls are guarded by per-call timeouts\n",
    "(via `signal.alarm`) and a process-wide memory limit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9f45c1d-ptb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import signal, resource, time, gc\n",
    "import numpy as np\n",
    "\n",
    "# ---- Safety: memory limit (8 GB virtual address space) ----\n",
    "_GB = 1024 ** 3\n",
    "_soft, _hard = resource.getrlimit(resource.RLIMIT_AS)\n",
    "resource.setrlimit(resource.RLIMIT_AS, (8 * _GB, _hard))\n",
    "\n",
    "class _Timeout(Exception):\n",
    "    pass\n",
    "\n",
    "def _alarm(signum, frame):\n",
    "    raise _Timeout()\n",
    "\n",
    "def timed(fn, timeout_s=30, label=''):\n",
    "    \"\"\"Run fn() with wall-clock timeout. Returns (result, elapsed_s) or (None, None).\"\"\"\n",
    "    prev = signal.signal(signal.SIGALRM, _alarm)\n",
    "    signal.alarm(timeout_s)\n",
    "    try:\n",
    "        t0 = time.perf_counter()\n",
    "        out = fn()\n",
    "        return out, time.perf_counter() - t0\n",
    "    except _Timeout:\n",
    "        print(f'  {label} TIMEOUT ({timeout_s}s)')\n",
    "        return None, None\n",
    "    except MemoryError:\n",
    "        print(f'  {label} OOM')\n",
    "        return None, None\n",
    "    except Exception as e:\n",
    "        print(f'  {label} ERROR: {type(e).__name__}: {e}')\n",
    "        return None, None\n",
    "    finally:\n",
    "        signal.alarm(0)\n",
    "        signal.signal(signal.SIGALRM, prev)\n",
    "\n",
    "# ---- Remap multi-char PTB symbols to single Unicode chars ----\n",
    "# PeekabooPrecover NFA uses string concatenation for buffers and indexes by\n",
    "# character position, which breaks for multi-character symbol names like '84'.\n",
    "def remap_fst_to_single_chars(fst):\n",
    "    from transduction.fst import FST as FSTClass\n",
    "    from transduction.fsa import EPSILON as _EPS\n",
    "    fwd, inv = {}, {}\n",
    "    code = 0xE000  # Unicode private-use area\n",
    "    for sym in sorted(fst.A | fst.B):\n",
    "        if sym == _EPS:\n",
    "            continue\n",
    "        fwd[sym] = chr(code)\n",
    "        inv[chr(code)] = sym\n",
    "        code += 1\n",
    "    new_fst = FSTClass()\n",
    "    for s in fst.start: new_fst.add_start(s)\n",
    "    for s in fst.stop:  new_fst.add_stop(s)\n",
    "    for s in fst.states:\n",
    "        for x, y, j in fst.arcs(s):\n",
    "            new_x = fwd.get(x, x) if x != _EPS else _EPS\n",
    "            new_y = fwd.get(y, y) if y != _EPS else _EPS\n",
    "            new_fst.add_arc(s, new_x, new_y, j)\n",
    "    return new_fst, fwd, inv\n",
    "\n",
    "# ---- Build PTB FST (requires pynini) ----\n",
    "from transduction.applications.ptb import build_ptb_fst_pynini, string_to_byte_strs, decode_ptb_output\n",
    "from transduction.fst import FST\n",
    "from transduction.fsa import EPSILON\n",
    "\n",
    "t0 = time.perf_counter()\n",
    "raw_fst = build_ptb_fst_pynini()\n",
    "ptb_fst, fwd_map, inv_map = remap_fst_to_single_chars(raw_fst)\n",
    "print(f'PTB FST built in {time.perf_counter()-t0:.1f}s: '\n",
    "      f'{len(ptb_fst.states)} states, |A|={len(ptb_fst.A)}, |B|={len(ptb_fst.B)}')\n",
    "\n",
    "# Generate target sequence\n",
    "text = \"The quick brown fox jumps over the lazy dog.\"\n",
    "byte_strs = string_to_byte_strs(text)\n",
    "remapped_input = tuple(fwd_map[s] for s in byte_strs)\n",
    "input_fst_obj = FST.from_string(remapped_input)\n",
    "output_fsa = (input_fst_obj @ ptb_fst).project(1)\n",
    "target_seq = list(next(output_fsa.language(tuple=True)))\n",
    "decoded = decode_ptb_output(tuple(inv_map.get(c, c) for c in target_seq))\n",
    "print(f'Target: {len(target_seq)} symbols')\n",
    "print(f'  {decoded!r}')\n",
    "\n",
    "# Train inner LM for TransducedLM benchmarks\n",
    "from transduction.lm.ngram import CharNgramLM\n",
    "source_alpha = ptb_fst.A - {EPSILON}\n",
    "train_text = (\n",
    "    \"The quick brown fox jumps over the lazy dog. \"\n",
    "    \"A stitch in time saves nine. To be or not to be, that is the question. \"\n",
    "    \"All that glitters is not gold. Actions speak louder than words. \"\n",
    "    \"Practice makes perfect. Where there is a will, there is a way. \"\n",
    ") * 3\n",
    "train_syms = [fwd_map[s] for s in string_to_byte_strs(train_text)]\n",
    "for sym in source_alpha:\n",
    "    train_syms.append(sym)\n",
    "inner_lm = CharNgramLM.train(train_syms, n=3, alpha=0.5)\n",
    "print(f'Inner LM: alphabet={len(inner_lm.alphabet)} symbols')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0c940a2-ptb",
   "metadata": {},
   "source": [
    "## Decomposition Scaling\n",
    "\n",
    "Per-step cost of computing decomposition at increasing target lengths.\n",
    "\n",
    "**Next-symbol prediction** (compute Q/R for all ~256 possible next output symbols):\n",
    "- **PeekabooState** (incremental, truncated buffer): chains `>>` + `.decomp`\n",
    "- **Peekaboo NR** (non-recursive, truncated buffer): fresh computation each step\n",
    "\n",
    "**Single-target decomposition** (Q/R for one specific target string):\n",
    "- **NonrecursiveDFADecomp** (unbounded buffer)\n",
    "\n",
    "Note: the next-symbol algorithms time out quickly on PTB because the powerset DFA\n",
    "with 256 output symbols is enormous.  NonrecursiveDFADecomp scales well because it\n",
    "only classifies states for a single target and prunes via universality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e7ab339-ptb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from transduction.peekaboo_incremental import PeekabooState, FstUniversality\n",
    "from transduction.peekaboo_nonrecursive import Peekaboo as PeekabooNR\n",
    "from transduction.dfa_decomp_nonrecursive import NonrecursiveDFADecomp\n",
    "\n",
    "MAX_N = min(25, len(target_seq))\n",
    "DECOMP_TIMEOUT = 30        # seconds per step (decomp-only algorithms)\n",
    "NEXTSYM_TIMEOUT = 120      # seconds per step (next-symbol algorithms — these are slow on PTB)\n",
    "\n",
    "decomp_results = defaultdict(list)  # name -> [(step, time_s)]\n",
    "\n",
    "# --- PeekabooState (incremental, next-symbol) ---\n",
    "# Computes decomposition for ALL ~256 next symbols simultaneously.\n",
    "# On PTB this is very slow (DFA state space explodes with 256 output symbols).\n",
    "print('PeekabooState (incremental, next-symbol):')\n",
    "univ = FstUniversality(ptb_fst)\n",
    "ps = PeekabooState(ptb_fst, '', parent=None, univ=univ)\n",
    "for i in range(min(5, MAX_N)):  # cap at 5 steps — each is very slow\n",
    "    y = target_seq[i]\n",
    "    def step(s=ps, y=y):\n",
    "        s2 = s >> y\n",
    "        _ = s2.decomp\n",
    "        return s2\n",
    "    out, t = timed(step, timeout_s=NEXTSYM_TIMEOUT, label=f'step {i+1}')\n",
    "    if t is None:\n",
    "        break\n",
    "    ps = out\n",
    "    decomp_results['PeekabooState'].append((i + 1, t))\n",
    "    print(f'  {i+1:2d}: {t*1000:8.1f} ms')\n",
    "gc.collect()\n",
    "\n",
    "# --- Peekaboo nonrecursive (next-symbol) ---\n",
    "print('\\nPeekaboo (nonrecursive, next-symbol):')\n",
    "for i in range(min(5, MAX_N)):\n",
    "    prefix = ''.join(target_seq[:i + 1])\n",
    "    def compute(p=prefix):\n",
    "        obj = PeekabooNR(ptb_fst, p)\n",
    "        _ = obj._results\n",
    "        return obj\n",
    "    _, t = timed(compute, timeout_s=NEXTSYM_TIMEOUT, label=f'step {i+1}')\n",
    "    if t is None:\n",
    "        break\n",
    "    decomp_results['PeekabooNR'].append((i + 1, t))\n",
    "    print(f'  {i+1:2d}: {t*1000:8.1f} ms')\n",
    "gc.collect()\n",
    "\n",
    "# --- NonrecursiveDFADecomp (single-target) ---\n",
    "# Computes Q/R for ONE specific target string.  Much faster because universality\n",
    "# pruning terminates the BFS early.\n",
    "print('\\nNonrecursiveDFADecomp (single-target):')\n",
    "for i in range(MAX_N):\n",
    "    prefix = ''.join(target_seq[:i + 1])\n",
    "    def compute(p=prefix):\n",
    "        return NonrecursiveDFADecomp(ptb_fst, p)\n",
    "    _, t = timed(compute, timeout_s=DECOMP_TIMEOUT, label=f'step {i+1}')\n",
    "    if t is None:\n",
    "        break\n",
    "    decomp_results['NonrecursiveDFADecomp'].append((i + 1, t))\n",
    "    print(f'  {i+1:2d}: {t*1000:8.1f} ms')\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86e628af-ptb",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "markers = iter(['o', 's', '^', 'D', 'v', 'P'])\n",
    "for name, data in sorted(decomp_results.items()):\n",
    "    if not data:\n",
    "        print(f'{name}: no data (timed out on first step)')\n",
    "        continue\n",
    "    steps, times = zip(*data)\n",
    "    ax.plot(steps, [t * 1000 for t in times], marker=next(markers, 'o'),\n",
    "            linestyle='-', label=name, markersize=4)\n",
    "    if len(data) < MAX_N:\n",
    "        ax.annotate(f'timeout', xy=(steps[-1], times[-1]*1000),\n",
    "                    xytext=(5, 5), textcoords='offset points',\n",
    "                    fontsize=8, color='red', alpha=0.7)\n",
    "\n",
    "ax.set_xlabel('Target prefix length')\n",
    "ax.set_ylabel('Time per step (ms)')\n",
    "ax.set_title('PTB Decomposition Scaling')\n",
    "ax.legend()\n",
    "ax.set_yscale('log')\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4445b584-ptb",
   "metadata": {},
   "source": [
    "## TransducedLM Scaling\n",
    "\n",
    "Per-step decode time for **TransducedLM** (two-phase: PeekabooState BFS\n",
    "decomposition, then LM-weighted search) vs **FusedTransducedLM** (single-pass:\n",
    "interleaved decomposition + LM search, no separate BFS).\n",
    "\n",
    "Each step includes both decomposition and LM search costs.  For TransducedLM,\n",
    "the PeekabooState BFS dominates (~35s per step on PTB).  FusedTransducedLM\n",
    "avoids the BFS entirely but builds the lazy DFA inline during search.\n",
    "\n",
    "Both use `max_steps=200`, `max_beam=100`, with a 120s timeout per step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d34b7551-ptb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transduction.lm.transduced import TransducedLM\n",
    "from transduction.lm.fused_transduced import FusedTransducedLM\n",
    "\n",
    "MAX_DECODE = 10              # number of decode steps\n",
    "MAX_SEARCH = 200             # max priority-queue steps per logp_next\n",
    "MAX_BEAM = 100               # max items carried forward\n",
    "LM_TIMEOUT = 120             # seconds per step\n",
    "\n",
    "# Expected runtime: ~15-20 min total (TransducedLM ~65s/step, FusedTransducedLM ~35s/step)\n",
    "\n",
    "lm_results = defaultdict(list)  # name -> [(step, time_s, logp)]\n",
    "\n",
    "for name, cls in [('TransducedLM', TransducedLM),\n",
    "                  ('FusedTransducedLM', FusedTransducedLM)]:\n",
    "    print(f'\\n{name} (max_steps={MAX_SEARCH}, max_beam={MAX_BEAM}):')\n",
    "    tlm = cls(inner_lm, ptb_fst, max_steps=MAX_SEARCH, max_beam=MAX_BEAM)\n",
    "    state = tlm.initial()\n",
    "    for i in range(min(MAX_DECODE, len(target_seq))):\n",
    "        y = target_seq[i]\n",
    "        def step(s=state, y=y):\n",
    "            lp = s.logp_next[y]\n",
    "            return s >> y, lp\n",
    "        out, t = timed(step, timeout_s=LM_TIMEOUT, label=f'step {i+1}')\n",
    "        if t is None:\n",
    "            break\n",
    "        state, lp = out\n",
    "        lm_results[name].append((i + 1, t, lp))\n",
    "        print(f'  {i+1:2d}: {t*1000:8.1f} ms  logp={lp:.4f}')\n",
    "    gc.collect()\n",
    "\n",
    "# Summary table\n",
    "print(f'\\n{\"Algorithm\":<25s} {\"Total (s)\":>10s} {\"Avg/step (s)\":>12s} {\"Steps\":>6s}')\n",
    "print('-' * 55)\n",
    "for name, data in sorted(lm_results.items()):\n",
    "    total = sum(t for _, t, _ in data)\n",
    "    avg = total / len(data)\n",
    "    print(f'{name:<25s} {total:10.1f} {avg:12.1f} {len(data):6d}')\n",
    "if len(lm_results) == 2:\n",
    "    names = sorted(lm_results.keys())\n",
    "    d0, d1 = lm_results[names[0]], lm_results[names[1]]\n",
    "    t0 = sum(t for _, t, _ in d0)\n",
    "    t1 = sum(t for _, t, _ in d1)\n",
    "    if t1 > 0:\n",
    "        print(f'\\nFused speedup (overall): {t0/t1:.2f}x')\n",
    "    # Exclude step 1 (amortization penalty for Fused)\n",
    "    if len(d0) > 1 and len(d1) > 1:\n",
    "        t0_skip1 = sum(t for _, t, _ in d0[1:])\n",
    "        t1_skip1 = sum(t for _, t, _ in d1[1:])\n",
    "        if t1_skip1 > 0:\n",
    "            print(f'Fused speedup (step 2+): {t0_skip1/t1_skip1:.2f}x')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7b94e52-ptb",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Left: time per step\n",
    "ax = axes[0]\n",
    "for name, data in sorted(lm_results.items()):\n",
    "    steps = [d[0] for d in data]\n",
    "    times = [d[1] for d in data]\n",
    "    ax.plot(steps, times, 'o-', label=name, markersize=4)\n",
    "ax.set_xlabel('Target step')\n",
    "ax.set_ylabel('Time per step (s)')\n",
    "ax.set_title(f'TransducedLM vs Fused (PTB, max_steps={MAX_SEARCH})')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Right: per-step speedup\n",
    "ax = axes[1]\n",
    "if len(lm_results) == 2:\n",
    "    names = sorted(lm_results.keys())\n",
    "    d0, d1 = lm_results[names[0]], lm_results[names[1]]\n",
    "    n = min(len(d0), len(d1))\n",
    "    steps = [d0[i][0] for i in range(n)]\n",
    "    speedups = [d0[i][1] / d1[i][1] if d1[i][1] > 0 else 0 for i in range(n)]\n",
    "    colors = ['#2ecc71' if s > 1 else '#e74c3c' for s in speedups]\n",
    "    ax.bar(steps, speedups, color=colors, alpha=0.7, edgecolor='white')\n",
    "    ax.axhline(1.0, color='black', linestyle='--', linewidth=0.8)\n",
    "    ax.set_xlabel('Target step')\n",
    "    ax.set_ylabel('Speedup (Original / Fused)')\n",
    "    ax.set_title('Per-step speedup (>1 = Fused faster)')\n",
    "    ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "    logp_diffs = [abs(d0[i][2] - d1[i][2]) for i in range(n)]\n",
    "    print(f'Max |logp| diff: {max(logp_diffs):.6f}')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05aa1351-ptb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Restore original memory limit\n",
    "resource.setrlimit(resource.RLIMIT_AS, (_soft, _hard))\n",
    "print('Memory limit restored.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68b948c1-ffee-4506-b6e1-7593dc4fb846",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
