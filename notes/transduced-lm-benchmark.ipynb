{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db48063f-59c2-415e-8651-1a22b4e8c47d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d548702-ptb",
   "metadata": {},
   "source": [
    "# TransducedLM vs FusedTransducedLM Benchmark\n",
    "\n",
    "Compares two approaches to computing next-symbol log-probabilities through\n",
    "an FST on the Penn Treebank tokenizer (~296 states, 257 input symbols):\n",
    "\n",
    "- **TransducedLM**: two-phase (PeekabooState BFS decomposition, then LM-weighted search)\n",
    "- **FusedTransducedLM**: single-pass (interleaved decomposition + LM search, no separate BFS)\n",
    "\n",
    "Uses a 3-gram CharNgramLM as the inner LM, with per-call timeouts and a\n",
    "process-wide memory limit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06b9abb0-b725-45da-b267-dd1ff9b062a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time, gc\n",
    "import numpy as np\n",
    "from transduction.applications.ptb import build_ptb_fst_pynini, string_to_byte_strs, decode_ptb_output\n",
    "from transduction.fsa import EPSILON\n",
    "from transduction.util import Timeout, timelimit, set_memory_limit\n",
    "set_memory_limit(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b42d7e07-0574-406c-85cd-4c0dba9e00e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "t0 = time.perf_counter()\n",
    "ptb_fst = build_ptb_fst_pynini()\n",
    "print(f'PTB FST built in {time.perf_counter()-t0:.1f}s: '\n",
    "      f'{len(ptb_fst.states)} states, |A|={len(ptb_fst.A)}, |B|={len(ptb_fst.B)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e1588c9-ae5c-4f48-8322-84f51d078307",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate target sequence via FST.transduce (PTB FST uses integer byte symbols directly)\n",
    "text = \"The quick brown fox jumps over the lazy dog.\"\n",
    "byte_ints = string_to_byte_strs(text)\n",
    "target_seq = list(ptb_fst.transduce(byte_ints))\n",
    "decoded = decode_ptb_output(tuple(target_seq))\n",
    "print(f'Target: {len(target_seq)} symbols')\n",
    "print(f'  {decoded!r}')\n",
    "\n",
    "# Train inner LM on integer byte symbols (CharNgramLM works with any hashable type).\n",
    "# Each sentence is a separate training instance so the model learns EOS.\n",
    "from transduction.lm.ngram import CharNgramLM\n",
    "source_alpha = ptb_fst.A - {EPSILON}\n",
    "train_sentences = [\n",
    "    \"The quick  brown   fox    jumps     over the lazy dog.\",\n",
    "    \"A stitch in time saves nine.\",\n",
    "    \"To be or not to be, that is the question.\",\n",
    "    \"All that glitters is not gold.\",\n",
    "    \"Actions speak louder than words.\",\n",
    "    \"Practice makes perfect.\",\n",
    "    \"Where there is a will, there is a way.\",\n",
    "] * 3\n",
    "train_instances = [list(string_to_byte_strs(s)) for s in train_sentences]\n",
    "inner_lm = CharNgramLM.train(train_instances, n=3, alpha=0.5, alphabet=source_alpha)\n",
    "print(f'Inner LM: alphabet={len(inner_lm.alphabet)} symbols')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "686bc4a4-292f-4240-891a-f07564691fc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebe9b19f-1826-43b3-a66c-a7e986835e87",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4445b584-ptb",
   "metadata": {},
   "source": [
    "## TransducedLM Scaling\n",
    "\n",
    "Per-step decode time for **TransducedLM** (two-phase: PeekabooState BFS\n",
    "decomposition, then LM-weighted search) vs **FusedTransducedLM** (single-pass:\n",
    "interleaved decomposition + LM search, no separate BFS).\n",
    "\n",
    "Each step includes both decomposition and LM search costs.  For TransducedLM,\n",
    "the PeekabooState BFS dominates (~35s per step on PTB).  FusedTransducedLM\n",
    "avoids the BFS entirely but builds the lazy DFA inline during search.\n",
    "\n",
    "Both use `max_steps=200`, `max_beam=100`, with a 120s timeout per step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a01af06-111e-4e17-9f02-b466723ed07a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from transduction.lm.transduced import TransducedLM\n",
    "from transduction.lm.fused_transduced import FusedTransducedLM\n",
    "\n",
    "MAX_DECODE = 100             # number of decode steps\n",
    "MAX_SEARCH = 200             # max priority-queue steps per logp_next\n",
    "MAX_BEAM = 20                # max items carried forward\n",
    "LM_TIMEOUT = 3               # seconds per step\n",
    "\n",
    "# Expected runtime: ~15-20 min total (TransducedLM ~65s/step, FusedTransducedLM ~35s/step)\n",
    "\n",
    "lm_results = defaultdict(list)  # name -> [(step, time_s, logp)]\n",
    "\n",
    "for name, cls in [\n",
    "    ('TransducedLM', TransducedLM),\n",
    "    ('FusedTransducedLM', FusedTransducedLM),\n",
    "]:\n",
    "    print(f'\\n{name} (max_steps={MAX_SEARCH}, max_beam={MAX_BEAM}):')\n",
    "    tlm = cls(inner_lm, ptb_fst, max_steps=MAX_SEARCH, max_beam=MAX_BEAM)\n",
    "    try:\n",
    "        with timelimit(LM_TIMEOUT):\n",
    "            state = tlm.initial()\n",
    "    except (Timeout, MemoryError) as e:\n",
    "        print(f'  initial() failed: {type(e).__name__}: {e}')\n",
    "        continue\n",
    "    for i in range(min(MAX_DECODE, len(target_seq))):\n",
    "        y = target_seq[i]\n",
    "        try:\n",
    "            with timelimit(LM_TIMEOUT):\n",
    "                t0 = time.perf_counter()\n",
    "                lp = state.logp_next[y]\n",
    "                state = state >> y\n",
    "                t1 = time.perf_counter()\n",
    "        except Timeout:\n",
    "            print(f'  step {i+1} TIMEOUT ({LM_TIMEOUT}s)')\n",
    "            break\n",
    "        except MemoryError:\n",
    "            print(f'  step {i+1} OOM')\n",
    "            break\n",
    "        elapsed = t1 - t0\n",
    "        lm_results[name].append((i + 1, elapsed, lp))\n",
    "        print(f'  {i+1:2d}: {elapsed*1000:8.1f} ms  logp={lp:.4f}')\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f67e7ce-b97b-4a00-87d1-f9e5026d5329",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84b61cb9-b345-418c-a48e-24d31b14cf8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Approximate posterior distribution over source strings given the target prefix.\n",
    "# TransducedState._repr_html_ groups particles by (source, DFA state),\n",
    "# normalizes log-weights, and renders an HTML table.\n",
    "state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "q453s1ijadf",
   "metadata": {},
   "source": [
    "## DFA State Inspection\n",
    "\n",
    "Each beam particle tracks a DFA state ID (an opaque `u32` integer from the Rust\n",
    "powerset construction). `decode_dfa_state()` maps these back to their NFA\n",
    "constituents: `(fst_state, target_buffer, truncated)` — matching the Python-side\n",
    "`PeekabooLookaheadNFA` representation.\n",
    "\n",
    "Notation: `(q, buf)` means FST state `q` with buffered target prefix `buf`.\n",
    "A `†` suffix marks truncated states."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7622ygpv6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transduction.lm.transduced import _format_nfa_set, _format_source_path\n",
    "\n",
    "ps = state._peekaboo_state\n",
    "print(f'Beam has {len(state._particles)} particles\\n')\n",
    "for i, p in enumerate(state._particles):\n",
    "    decoded = ps.decode_dfa_state(p.dfa_state)\n",
    "    source = _format_source_path(p.lm_state)\n",
    "    print(f'  particle {i}: dfa_id={p.dfa_state}  w={p.log_weight:.3f}')\n",
    "    print(f'    source: {source!r}')\n",
    "    print(f'    NFA set: {_format_nfa_set(decoded)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gywn43hfdkd",
   "metadata": {},
   "source": [
    "### Q/R FSA Visualization\n",
    "\n",
    "For each target symbol `y`, the decomposition produces a **quotient** FSA Q(y)\n",
    "(accepts source prefixes where *all* continuations produce `y` next) and a\n",
    "**remainder** FSA R(y) (the leftover after removing the quotient).\n",
    "\n",
    "Below we render Q and R for a few symbols, highlighting beam particle states in\n",
    "blue.  States are labeled with their decoded NFA sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1imq9cat454h",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, HTML, SVG\n",
    "\n",
    "ps = state._peekaboo_state\n",
    "particle_states = {p.dfa_state for p in state._particles}\n",
    "\n",
    "# Decoded-label formatter and beam-state highlighter\n",
    "decode_cache = {}\n",
    "def fmt_node(s):\n",
    "    if s not in decode_cache:\n",
    "        try:\n",
    "            decode_cache[s] = _format_nfa_set(ps.decode_dfa_state(s))\n",
    "        except Exception:\n",
    "            decode_cache[s] = str(s)\n",
    "    return decode_cache[s]\n",
    "\n",
    "def sty_node(s):\n",
    "    if s in particle_states:\n",
    "        return {'fillcolor': '#ADD8E6', 'style': 'filled,rounded'}\n",
    "    return {}\n",
    "\n",
    "# Pick a few symbols to visualize (prefer ones with non-trivial Q/R)\n",
    "decomp = ps.decomp\n",
    "shown = 0\n",
    "MAX_SHOW = 5\n",
    "for y in sorted(decomp.keys(), key=repr):\n",
    "    if shown >= MAX_SHOW:\n",
    "        break\n",
    "    try:\n",
    "        q_fsa, r_fsa = ps.build_qr_fsa(y)\n",
    "    except Exception:\n",
    "        continue\n",
    "    if not q_fsa.states and not r_fsa.states:\n",
    "        continue\n",
    "\n",
    "    y_label = repr(y)\n",
    "    display(HTML(f'<h4>y = {y_label}</h4>'))\n",
    "    if q_fsa.states:\n",
    "        display(HTML(f'<b>Q({y_label})</b> — {len(q_fsa.states)} states'))\n",
    "        g = q_fsa.graphviz(fmt_node=fmt_node, sty_node=sty_node)\n",
    "        display(SVG(g._repr_image_svg_xml()))\n",
    "    if r_fsa.states:\n",
    "        display(HTML(f'<b>R({y_label})</b> — {len(r_fsa.states)} states'))\n",
    "        g = r_fsa.graphviz(fmt_node=fmt_node, sty_node=sty_node)\n",
    "        display(SVG(g._repr_image_svg_xml()))\n",
    "    shown += 1\n",
    "\n",
    "if shown == 0:\n",
    "    print('No non-trivial Q/R FSAs for current decomposition symbols.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ogk3boj5f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Most likely next symbols from this state\n",
    "logp = state.logp_next\n",
    "syms = sorted(logp.keys(), key=lambda s: logp[s], reverse=True)[:15]\n",
    "print(f\"{'symbol':>8s}  {'char':>6s}  {'logp':>8s}  {'prob':>8s}\")\n",
    "print(\"-\" * 36)\n",
    "for s in syms:\n",
    "    if isinstance(s, int):\n",
    "        ch = chr(s) if 32 <= s <= 126 else f'\\\\x{s:02x}'\n",
    "    else:\n",
    "        ch = repr(s)\n",
    "    print(f\"{str(s):>8s}  {ch:>6s}  {logp[s]:8.4f}  {np.exp(logp[s]):8.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "773d17c0-5a38-49e3-bf8f-c92fd22abdd3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb1e75ae-e147-47f1-a549-6e3fae0b75f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary table\n",
    "print(f'\\n{\"Algorithm\":<25s} {\"Total (s)\":>10s} {\"Avg/step (s)\":>12s} {\"Steps\":>6s}')\n",
    "print('-' * 55)\n",
    "for name, data in sorted(lm_results.items()):\n",
    "    total = sum(t for _, t, _ in data)\n",
    "    avg = total / len(data)\n",
    "    print(f'{name:<25s} {total:10.1f} {avg:12.1f} {len(data):6d}')\n",
    "if len(lm_results) == 2:\n",
    "    names = sorted(lm_results.keys())\n",
    "    d0, d1 = lm_results[names[0]], lm_results[names[1]]\n",
    "    t0 = sum(t for _, t, _ in d0)\n",
    "    t1 = sum(t for _, t, _ in d1)\n",
    "    if t1 > 0:\n",
    "        print(f'\\nFused speedup (overall): {t0/t1:.2f}x')\n",
    "    # Exclude step 1 (amortization penalty for Fused)\n",
    "    if len(d0) > 1 and len(d1) > 1:\n",
    "        t0_skip1 = sum(t for _, t, _ in d0[1:])\n",
    "        t1_skip1 = sum(t for _, t, _ in d1[1:])\n",
    "        if t1_skip1 > 0:\n",
    "            print(f'Fused speedup (step 2+): {t0_skip1/t1_skip1:.2f}x')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7b94e52-ptb",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Left: time per step\n",
    "ax = axes[0]\n",
    "for name, data in sorted(lm_results.items()):\n",
    "    steps = [d[0] for d in data]\n",
    "    times = [d[1] for d in data]\n",
    "    ax.plot(steps, times, 'o-', label=name, markersize=4)\n",
    "ax.set_xlabel('Target step')\n",
    "ax.set_ylabel('Time per step (s)')\n",
    "ax.set_title(f'TransducedLM vs Fused (PTB, max_steps={MAX_SEARCH})')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Right: per-step speedup\n",
    "ax = axes[1]\n",
    "if len(lm_results) == 2:\n",
    "    names = sorted(lm_results.keys())\n",
    "    d0, d1 = lm_results[names[0]], lm_results[names[1]]\n",
    "    n = min(len(d0), len(d1))\n",
    "    steps = [d0[i][0] for i in range(n)]\n",
    "    speedups = [d0[i][1] / d1[i][1] if d1[i][1] > 0 else 0 for i in range(n)]\n",
    "    colors = ['#2ecc71' if s > 1 else '#e74c3c' for s in speedups]\n",
    "    ax.bar(steps, speedups, color=colors, alpha=0.7, edgecolor='white')\n",
    "    ax.axhline(1.0, color='black', linestyle='--', linewidth=0.8)\n",
    "    ax.set_xlabel('Target step')\n",
    "    ax.set_ylabel('Speedup (Original / Fused)')\n",
    "    ax.set_title('Per-step speedup (>1 = Fused faster)')\n",
    "    ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "    logp_diffs = [abs(d0[i][2] - d1[i][2]) for i in range(n)]\n",
    "    print(f'Max |logp| diff: {max(logp_diffs):.6f}')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75fbae41-3a99-491e-8052-811b3b06a8ec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
