{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "db48063f-59c2-415e-8651-1a22b4e8c47d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d548702-ptb",
   "metadata": {},
   "source": [
    "# TransducedLM vs FusedTransducedLM Benchmark\n",
    "\n",
    "Compares two approaches to computing next-symbol log-probabilities through\n",
    "an FST on the Penn Treebank tokenizer (~296 states, 257 input symbols):\n",
    "\n",
    "- **TransducedLM**: two-phase (PeekabooState BFS decomposition, then LM-weighted search)\n",
    "- **FusedTransducedLM**: single-pass (interleaved decomposition + LM search, no separate BFS)\n",
    "\n",
    "Uses a 3-gram CharNgramLM as the inner LM, with per-call timeouts and a\n",
    "process-wide memory limit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "06b9abb0-b725-45da-b267-dd1ff9b062a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time, gc\n",
    "import numpy as np\n",
    "\n",
    "from transduction.util import Timeout, timelimit, set_memory_limit\n",
    "\n",
    "# ---- Safety: memory limit (8 GB virtual address space) ----\n",
    "import resource\n",
    "_soft, _hard = resource.getrlimit(resource.RLIMIT_AS)\n",
    "set_memory_limit(8)\n",
    "\n",
    "# ---- Remap multi-char PTB symbols to single Unicode chars via map_labels ----\n",
    "from transduction.applications.ptb import build_ptb_fst_pynini, string_to_byte_strs, decode_ptb_output\n",
    "from transduction.fst import FST\n",
    "from transduction.fsa import EPSILON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b42d7e07-0574-406c-85cd-4c0dba9e00e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Composing PTB rules...\n",
      "Core PTB FST: 310 states\n",
      "Final pynini FST: 296 states\n",
      "Converting to native FST...\n",
      "Native FST: 296 states, 23723 arcs\n",
      "  eps: 108 in, 352 out\n",
      "  MARKER: 0 in, 0 out\n",
      "  [EOS]: 0 in, 0 out\n",
      "PTB FST built in 31.1s: 296 states, |A|=257, |B|=256\n"
     ]
    }
   ],
   "source": [
    "t0 = time.perf_counter()\n",
    "raw_fst = build_ptb_fst_pynini()\n",
    "\n",
    "# Build forward/inverse maps for all non-epsilon symbols\n",
    "fwd_map, inv_map = {}, {}\n",
    "code = 0xE000  # Unicode private-use area\n",
    "for sym in sorted((raw_fst.A | raw_fst.B) - {EPSILON}):\n",
    "    fwd_map[sym] = chr(code)\n",
    "    inv_map[chr(code)] = sym\n",
    "    code += 1\n",
    "\n",
    "ptb_fst = raw_fst.map_labels(lambda a, b: (fwd_map.get(a, a), fwd_map.get(b, b)))\n",
    "print(f'PTB FST built in {time.perf_counter()-t0:.1f}s: '\n",
    "      f'{len(ptb_fst.states)} states, |A|={len(ptb_fst.A)}, |B|={len(ptb_fst.B)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7e1588c9-ae5c-4f48-8322-84f51d078307",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target: 45 symbols\n",
      "  'The quick brown fox jumps over the lazy dog .'\n",
      "Inner LM: alphabet=257 symbols\n"
     ]
    }
   ],
   "source": [
    "# Generate target sequence\n",
    "text = \"The quick brown fox jumps over the lazy dog.\"\n",
    "byte_strs = string_to_byte_strs(text)\n",
    "remapped_input = tuple(fwd_map[s] for s in byte_strs)\n",
    "input_fst_obj = FST.from_string(remapped_input)\n",
    "output_fsa = (input_fst_obj @ ptb_fst).project(1)\n",
    "target_seq = list(next(output_fsa.language()))\n",
    "decoded = decode_ptb_output(tuple(inv_map.get(c, c) for c in target_seq))\n",
    "print(f'Target: {len(target_seq)} symbols')\n",
    "print(f'  {decoded!r}')\n",
    "\n",
    "# Train inner LM for TransducedLM benchmarks\n",
    "from transduction.lm.ngram import CharNgramLM\n",
    "source_alpha = ptb_fst.A - {EPSILON}\n",
    "train_text = (\n",
    "    \"The quick brown fox jumps over the lazy dog. \"\n",
    "    \"A stitch in time saves nine. To be or not to be, that is the question. \"\n",
    "    \"All that glitters is not gold. Actions speak louder than words. \"\n",
    "    \"Practice makes perfect. Where there is a will, there is a way. \"\n",
    ") * 3\n",
    "train_syms = [fwd_map[s] for s in string_to_byte_strs(train_text)]\n",
    "for sym in source_alpha:\n",
    "    train_syms.append(sym)\n",
    "inner_lm = CharNgramLM.train(train_syms, n=3, alpha=0.5)\n",
    "print(f'Inner LM: alphabet={len(inner_lm.alphabet)} symbols')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4445b584-ptb",
   "metadata": {},
   "source": [
    "## TransducedLM Scaling\n",
    "\n",
    "Per-step decode time for **TransducedLM** (two-phase: PeekabooState BFS\n",
    "decomposition, then LM-weighted search) vs **FusedTransducedLM** (single-pass:\n",
    "interleaved decomposition + LM search, no separate BFS).\n",
    "\n",
    "Each step includes both decomposition and LM search costs.  For TransducedLM,\n",
    "the PeekabooState BFS dominates (~35s per step on PTB).  FusedTransducedLM\n",
    "avoids the BFS entirely but builds the lazy DFA inline during search.\n",
    "\n",
    "Both use `max_steps=200`, `max_beam=100`, with a 120s timeout per step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9a01af06-111e-4e17-9f02-b466723ed07a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TransducedLM (max_steps=200, max_beam=10):\n",
      "hello\n",
      "  initial() failed: Timeout: Call took longer than 3 seconds.\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "from transduction.lm.transduced import TransducedLM\n",
    "from transduction.lm.fused_transduced import FusedTransducedLM\n",
    "\n",
    "MAX_DECODE = 10              # number of decode steps\n",
    "MAX_SEARCH = 200             # max priority-queue steps per logp_next\n",
    "MAX_BEAM = 10                # max items carried forward\n",
    "LM_TIMEOUT = 3               # seconds per step\n",
    "\n",
    "# Expected runtime: ~15-20 min total (TransducedLM ~65s/step, FusedTransducedLM ~35s/step)\n",
    "\n",
    "lm_results = defaultdict(list)  # name -> [(step, time_s, logp)]\n",
    "\n",
    "for name, cls in [('TransducedLM', TransducedLM),\n",
    "                  #('FusedTransducedLM', FusedTransducedLM)\n",
    "                 ]:\n",
    "    print(f'\\n{name} (max_steps={MAX_SEARCH}, max_beam={MAX_BEAM}):')\n",
    "    tlm = cls(inner_lm, ptb_fst, max_steps=MAX_SEARCH, max_beam=MAX_BEAM)\n",
    "    try:\n",
    "        with timelimit(LM_TIMEOUT):\n",
    "            state = tlm.initial()\n",
    "    except (Timeout, MemoryError) as e:\n",
    "        print(f'  initial() failed: {type(e).__name__}: {e}')\n",
    "        continue\n",
    "    for i in range(min(MAX_DECODE, len(target_seq))):\n",
    "        y = target_seq[i]\n",
    "        try:\n",
    "            with timelimit(LM_TIMEOUT):\n",
    "                t0 = time.perf_counter()\n",
    "                lp = state.logp_next[y]\n",
    "                state = state >> y\n",
    "                t1 = time.perf_counter()\n",
    "        except Timeout:\n",
    "            print(f'  step {i+1} TIMEOUT ({LM_TIMEOUT}s)')\n",
    "            break\n",
    "        except MemoryError:\n",
    "            print(f'  step {i+1} OOM')\n",
    "            break\n",
    "        elapsed = t1 - t0\n",
    "        lm_results[name].append((i + 1, elapsed, lp))\n",
    "        print(f'  {i+1:2d}: {elapsed*1000:8.1f} ms  logp={lp:.4f}')\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb1e75ae-e147-47f1-a549-6e3fae0b75f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary table\n",
    "print(f'\\n{\"Algorithm\":<25s} {\"Total (s)\":>10s} {\"Avg/step (s)\":>12s} {\"Steps\":>6s}')\n",
    "print('-' * 55)\n",
    "for name, data in sorted(lm_results.items()):\n",
    "    total = sum(t for _, t, _ in data)\n",
    "    avg = total / len(data)\n",
    "    print(f'{name:<25s} {total:10.1f} {avg:12.1f} {len(data):6d}')\n",
    "if len(lm_results) == 2:\n",
    "    names = sorted(lm_results.keys())\n",
    "    d0, d1 = lm_results[names[0]], lm_results[names[1]]\n",
    "    t0 = sum(t for _, t, _ in d0)\n",
    "    t1 = sum(t for _, t, _ in d1)\n",
    "    if t1 > 0:\n",
    "        print(f'\\nFused speedup (overall): {t0/t1:.2f}x')\n",
    "    # Exclude step 1 (amortization penalty for Fused)\n",
    "    if len(d0) > 1 and len(d1) > 1:\n",
    "        t0_skip1 = sum(t for _, t, _ in d0[1:])\n",
    "        t1_skip1 = sum(t for _, t, _ in d1[1:])\n",
    "        if t1_skip1 > 0:\n",
    "            print(f'Fused speedup (step 2+): {t0_skip1/t1_skip1:.2f}x')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7b94e52-ptb",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Left: time per step\n",
    "ax = axes[0]\n",
    "for name, data in sorted(lm_results.items()):\n",
    "    steps = [d[0] for d in data]\n",
    "    times = [d[1] for d in data]\n",
    "    ax.plot(steps, times, 'o-', label=name, markersize=4)\n",
    "ax.set_xlabel('Target step')\n",
    "ax.set_ylabel('Time per step (s)')\n",
    "ax.set_title(f'TransducedLM vs Fused (PTB, max_steps={MAX_SEARCH})')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Right: per-step speedup\n",
    "ax = axes[1]\n",
    "if len(lm_results) == 2:\n",
    "    names = sorted(lm_results.keys())\n",
    "    d0, d1 = lm_results[names[0]], lm_results[names[1]]\n",
    "    n = min(len(d0), len(d1))\n",
    "    steps = [d0[i][0] for i in range(n)]\n",
    "    speedups = [d0[i][1] / d1[i][1] if d1[i][1] > 0 else 0 for i in range(n)]\n",
    "    colors = ['#2ecc71' if s > 1 else '#e74c3c' for s in speedups]\n",
    "    ax.bar(steps, speedups, color=colors, alpha=0.7, edgecolor='white')\n",
    "    ax.axhline(1.0, color='black', linestyle='--', linewidth=0.8)\n",
    "    ax.set_xlabel('Target step')\n",
    "    ax.set_ylabel('Speedup (Original / Fused)')\n",
    "    ax.set_title('Per-step speedup (>1 = Fused faster)')\n",
    "    ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "    logp_diffs = [abs(d0[i][2] - d1[i][2]) for i in range(n)]\n",
    "    print(f'Max |logp| diff: {max(logp_diffs):.6f}')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05aa1351-ptb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Restore original memory limit\n",
    "resource.setrlimit(resource.RLIMIT_AS, (_soft, _hard))\n",
    "print('Memory limit restored.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "321d753a-74f5-4bd0-9c11-3aaef9f6c186",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "557de0f9-fe81-46b7-83aa-b0fd5903406a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
