{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "db48063f-59c2-415e-8651-1a22b4e8c47d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d548702-ptb",
   "metadata": {},
   "source": [
    "# TransducedLM vs FusedTransducedLM Benchmark\n",
    "\n",
    "Compares two approaches to computing next-symbol log-probabilities through\n",
    "an FST on the Penn Treebank tokenizer (~296 states, 257 input symbols):\n",
    "\n",
    "- **TransducedLM**: two-phase (PeekabooState BFS decomposition, then LM-weighted search)\n",
    "- **FusedTransducedLM**: single-pass (interleaved decomposition + LM search, no separate BFS)\n",
    "\n",
    "Uses a 3-gram CharNgramLM as the inner LM, with per-call timeouts and a\n",
    "process-wide memory limit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "06b9abb0-b725-45da-b267-dd1ff9b062a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time, gc\n",
    "import numpy as np\n",
    "from transduction.applications.ptb import build_ptb_fst_pynini, string_to_byte_strs, decode_ptb_output\n",
    "from transduction.fsa import EPSILON\n",
    "from transduction.util import Timeout, timelimit, set_memory_limit\n",
    "set_memory_limit(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b42d7e07-0574-406c-85cd-4c0dba9e00e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Composing PTB rules...\n",
      "Core PTB FST: 310 states\n",
      "Final pynini FST: 296 states\n",
      "Converting to native FST...\n",
      "Native FST: 296 states, 23723 arcs\n",
      "  eps: 108 in, 352 out\n",
      "  MARKER: 0 in, 0 out\n",
      "  [EOS]: 0 in, 0 out\n",
      "PTB FST built in 31.7s: 296 states, |A|=257, |B|=256\n"
     ]
    }
   ],
   "source": [
    "t0 = time.perf_counter()\n",
    "ptb_fst = build_ptb_fst_pynini()\n",
    "print(f'PTB FST built in {time.perf_counter()-t0:.1f}s: '\n",
    "      f'{len(ptb_fst.states)} states, |A|={len(ptb_fst.A)}, |B|={len(ptb_fst.B)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7e1588c9-ae5c-4f48-8322-84f51d078307",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target: 45 symbols\n",
      "  'The quick brown fox jumps over the lazy dog .'\n",
      "Inner LM: alphabet=257 symbols\n"
     ]
    }
   ],
   "source": [
    "# Generate target sequence via FST.transduce (PTB FST uses integer byte symbols directly)\n",
    "text = \"The quick brown fox jumps over the lazy dog.\"\n",
    "byte_ints = string_to_byte_strs(text)\n",
    "target_seq = list(ptb_fst.transduce(byte_ints))\n",
    "decoded = decode_ptb_output(tuple(target_seq))\n",
    "print(f'Target: {len(target_seq)} symbols')\n",
    "print(f'  {decoded!r}')\n",
    "\n",
    "# Train inner LM on integer byte symbols (CharNgramLM works with any hashable type).\n",
    "# Each sentence is a separate training instance so the model learns EOS.\n",
    "from transduction.lm.ngram import CharNgramLM\n",
    "source_alpha = ptb_fst.A - {EPSILON}\n",
    "train_sentences = [\n",
    "    \"The quick  brown   fox    jumps     over the lazy dog.\",\n",
    "    \"A stitch in time saves nine.\",\n",
    "    \"To be or not to be, that is the question.\",\n",
    "    \"All that glitters is not gold.\",\n",
    "    \"Actions speak louder than words.\",\n",
    "    \"Practice makes perfect.\",\n",
    "    \"Where there is a will, there is a way.\",\n",
    "] * 3\n",
    "train_instances = [list(string_to_byte_strs(s)) for s in train_sentences]\n",
    "inner_lm = CharNgramLM.train(train_instances, n=3, alpha=0.5, alphabet=source_alpha)\n",
    "print(f'Inner LM: alphabet={len(inner_lm.alphabet)} symbols')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "686bc4a4-292f-4240-891a-f07564691fc9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The quick brown fox jumps over the lazy dog .'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebe9b19f-1826-43b3-a66c-a7e986835e87",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4445b584-ptb",
   "metadata": {},
   "source": [
    "## TransducedLM Scaling\n",
    "\n",
    "Per-step decode time for **TransducedLM** (two-phase: PeekabooState BFS\n",
    "decomposition, then LM-weighted search) vs **FusedTransducedLM** (single-pass:\n",
    "interleaved decomposition + LM search, no separate BFS).\n",
    "\n",
    "Each step includes both decomposition and LM search costs.  For TransducedLM,\n",
    "the PeekabooState BFS dominates (~35s per step on PTB).  FusedTransducedLM\n",
    "avoids the BFS entirely but builds the lazy DFA inline during search.\n",
    "\n",
    "Both use `max_steps=200`, `max_beam=100`, with a 120s timeout per step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a01af06-111e-4e17-9f02-b466723ed07a",
   "metadata": {},
   "outputs": [],
   "source": "from collections import defaultdict\nfrom transduction.lm.transduced import TransducedLM\nfrom transduction.lm.fused_transduced import FusedTransducedLM\n\nMAX_DECODE = 100             # number of decode steps\nMAX_SEARCH = 200             # max priority-queue steps per logp_next\nMAX_BEAM = 20                # max particles carried forward\nLM_TIMEOUT = 3               # seconds per step\n\n# Expected runtime: ~15-20 min total (TransducedLM ~65s/step, FusedTransducedLM ~35s/step)\n\nlm_results = defaultdict(list)  # name -> [(step, time_s, logp)]\n\nfor name, cls in [\n    ('TransducedLM', TransducedLM),\n    ('FusedTransducedLM', FusedTransducedLM),\n]:\n    print(f'\\n{name} (max_steps={MAX_SEARCH}, max_beam={MAX_BEAM}):')\n    tlm = cls(inner_lm, ptb_fst, max_steps=MAX_SEARCH, max_beam=MAX_BEAM)\n    try:\n        with timelimit(LM_TIMEOUT):\n            state = tlm.initial()\n    except (Timeout, MemoryError) as e:\n        print(f'  initial() failed: {type(e).__name__}: {e}')\n        continue\n    for i in range(min(MAX_DECODE, len(target_seq))):\n        y = target_seq[i]\n        try:\n            with timelimit(LM_TIMEOUT):\n                t0 = time.perf_counter()\n                lp = state.logp_next[y]\n                state = state >> y\n                t1 = time.perf_counter()\n        except Timeout:\n            print(f'  step {i+1} TIMEOUT ({LM_TIMEOUT}s)')\n            break\n        except MemoryError:\n            print(f'  step {i+1} OOM')\n            break\n        elapsed = t1 - t0\n        lm_results[name].append((i + 1, elapsed, lp))\n        print(f'  {i+1:2d}: {elapsed*1000:8.1f} ms  logp={lp:.4f}')\n    gc.collect()"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7f67e7ce-b97b-4a00-87d1-f9e5026d5329",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "transduction.lm.fused_transduced.FusedTransducedState"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "84b61cb9-b345-418c-a48e-24d31b14cf8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<b>FusedTransducedState</b> target='8410410125811311710599107258981141111191102581021111202581061171091121152581111181011142581161041012581089712212125810011110325846', K=1, logp=-159.2105<br><table class=\"fmt-table\" style=\"border-collapse:collapse;\"><thead><tr><th>Source prefix</th><th>DFA state</th><th>Count</th><th>log w</th><th>p(x|y)</th></tr></thead><tbody><tr><td style=\"vertical-align:top;text-align:left\"><pre>&#x27;The quick brown fox jumps over the lazy dog...&#x27;</pre></td><td style=\"vertical-align:top\"><pre>{(124, (84,104,101,258…)†), (201, (84,104,101,258…)†), (57, (84,104,101,258…)†)}</pre></td><td style=\"vertical-align:top\"><pre>1</pre></td><td style=\"vertical-align:top\"><pre>-173.80</pre></td><td style=\"vertical-align:top\"><pre>1.0000</pre></td></tr></tbody></table>"
      ],
      "text/plain": [
       "FusedTransducedState(target=(84, 104, 101, 258, 113, 117, 105, 99, 107, 258, 98, 114, 111, 119, 110, 258, 102, 111, 120, 258, 106, 117, 109, 112, 115, 258, 111, 118, 101, 114, 258, 116, 104, 101, 258, 108, 97, 122, 121, 258, 100, 111, 103, 258, 46))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Approximate posterior distribution over source strings given the target prefix.\n",
    "# TransducedState._repr_html_ groups particles by (source, DFA state),\n",
    "# normalizes log-weights, and renders an HTML table.\n",
    "state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "q453s1ijadf",
   "metadata": {},
   "source": "## DFA State Inspection\n\nEach particle tracks a DFA state ID (an opaque `u32` integer from the Rust\npowerset construction). `decode_dfa_state()` maps these back to their NFA\nconstituents: `(fst_state, target_buffer, truncated)` — matching the Python-side\n`PeekabooLookaheadNFA` representation.\n\nNotation: `(q, buf)` means FST state `q` with buffered target prefix `buf`.\nA `†` suffix marks truncated states."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7622ygpv6",
   "metadata": {},
   "outputs": [],
   "source": "from transduction.lm.transduced import _format_nfa_set, _format_source_path\n\nprint(f'{len(state._particles)} particles\\n')\nfor i, p in enumerate(state._particles):\n    decoded = state.decode_dfa_state(p.dfa_state)\n    source = _format_source_path(p.lm_state)\n    print(f'  particle {i}: dfa_id={p.dfa_state}  w={p.log_weight:.3f}')\n    print(f'    source: {source!r}')\n    print(f'    NFA set: {_format_nfa_set(decoded)}')"
  },
  {
   "cell_type": "markdown",
   "id": "gywn43hfdkd",
   "metadata": {},
   "source": "### Q/R FSA Visualization\n\nFor each target symbol `y`, the decomposition produces a **quotient** FSA Q(y)\n(accepts source prefixes where *all* continuations produce `y` next) and a\n**remainder** FSA R(y) (the leftover after removing the quotient).\n\nBelow we render Q and R for a few symbols, highlighting particle states in\nblue.  States are labeled with their decoded NFA sets."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1imq9cat454h",
   "metadata": {},
   "outputs": [],
   "source": "from IPython.display import display, HTML, SVG\n\nparticle_states = {p.dfa_state for p in state._particles}\n\n# Decoded-label formatter and particle-state highlighter\ndecode_cache = {}\ndef fmt_node(s):\n    if s not in decode_cache:\n        try:\n            decode_cache[s] = _format_nfa_set(state.decode_dfa_state(s))\n        except Exception:\n            decode_cache[s] = str(s)\n    return decode_cache[s]\n\ndef sty_node(s):\n    if s in particle_states:\n        return {'fillcolor': '#ADD8E6', 'style': 'filled,rounded'}\n    return {}\n\n# Q/R visualization requires a PeekabooState (TransducedState only)\nif hasattr(state, '_peekaboo_state'):\n    ps = state._peekaboo_state\n    decomp = ps.decomp\n    shown = 0\n    MAX_SHOW = 5\n    for y in sorted(decomp.keys(), key=repr):\n        if shown >= MAX_SHOW:\n            break\n        try:\n            q_fsa, r_fsa = ps.build_qr_fsa(y)\n        except Exception:\n            continue\n        if not q_fsa.states and not r_fsa.states:\n            continue\n\n        y_label = repr(y)\n        display(HTML(f'<h4>y = {y_label}</h4>'))\n        if q_fsa.states:\n            display(HTML(f'<b>Q({y_label})</b> — {len(q_fsa.states)} states'))\n            g = q_fsa.graphviz(fmt_node=fmt_node, sty_node=sty_node)\n            display(SVG(g._repr_image_svg_xml()))\n        if r_fsa.states:\n            display(HTML(f'<b>R({y_label})</b> — {len(r_fsa.states)} states'))\n            g = r_fsa.graphviz(fmt_node=fmt_node, sty_node=sty_node)\n            display(SVG(g._repr_image_svg_xml()))\n        shown += 1\n\n    if shown == 0:\n        print('No non-trivial Q/R FSAs for current decomposition symbols.')\nelse:\n    print('Q/R FSA visualization not available for FusedTransducedState '\n          '(no pre-computed decomposition).')"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ogk3boj5f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Most likely next symbols from this state\n",
    "logp = state.logp_next\n",
    "syms = sorted(logp.keys(), key=lambda s: logp[s], reverse=True)[:15]\n",
    "print(f\"{'symbol':>8s}  {'char':>6s}  {'logp':>8s}  {'prob':>8s}\")\n",
    "print(\"-\" * 36)\n",
    "for s in syms:\n",
    "    if isinstance(s, int):\n",
    "        ch = chr(s) if 32 <= s <= 126 else f'\\\\x{s:02x}'\n",
    "    else:\n",
    "        ch = repr(s)\n",
    "    print(f\"{str(s):>8s}  {ch:>6s}  {logp[s]:8.4f}  {np.exp(logp[s]):8.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "773d17c0-5a38-49e3-bf8f-c92fd22abdd3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb1e75ae-e147-47f1-a549-6e3fae0b75f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary table\n",
    "print(f'\\n{\"Algorithm\":<25s} {\"Total (s)\":>10s} {\"Avg/step (s)\":>12s} {\"Steps\":>6s}')\n",
    "print('-' * 55)\n",
    "for name, data in sorted(lm_results.items()):\n",
    "    total = sum(t for _, t, _ in data)\n",
    "    avg = total / len(data)\n",
    "    print(f'{name:<25s} {total:10.1f} {avg:12.1f} {len(data):6d}')\n",
    "if len(lm_results) == 2:\n",
    "    names = sorted(lm_results.keys())\n",
    "    d0, d1 = lm_results[names[0]], lm_results[names[1]]\n",
    "    t0 = sum(t for _, t, _ in d0)\n",
    "    t1 = sum(t for _, t, _ in d1)\n",
    "    if t1 > 0:\n",
    "        print(f'\\nFused speedup (overall): {t0/t1:.2f}x')\n",
    "    # Exclude step 1 (amortization penalty for Fused)\n",
    "    if len(d0) > 1 and len(d1) > 1:\n",
    "        t0_skip1 = sum(t for _, t, _ in d0[1:])\n",
    "        t1_skip1 = sum(t for _, t, _ in d1[1:])\n",
    "        if t1_skip1 > 0:\n",
    "            print(f'Fused speedup (step 2+): {t0_skip1/t1_skip1:.2f}x')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7b94e52-ptb",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Left: time per step\n",
    "ax = axes[0]\n",
    "for name, data in sorted(lm_results.items()):\n",
    "    steps = [d[0] for d in data]\n",
    "    times = [d[1] for d in data]\n",
    "    ax.plot(steps, times, 'o-', label=name, markersize=4)\n",
    "ax.set_xlabel('Target step')\n",
    "ax.set_ylabel('Time per step (s)')\n",
    "ax.set_title(f'TransducedLM vs Fused (PTB, max_steps={MAX_SEARCH})')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Right: per-step speedup\n",
    "ax = axes[1]\n",
    "if len(lm_results) == 2:\n",
    "    names = sorted(lm_results.keys())\n",
    "    d0, d1 = lm_results[names[0]], lm_results[names[1]]\n",
    "    n = min(len(d0), len(d1))\n",
    "    steps = [d0[i][0] for i in range(n)]\n",
    "    speedups = [d0[i][1] / d1[i][1] if d1[i][1] > 0 else 0 for i in range(n)]\n",
    "    colors = ['#2ecc71' if s > 1 else '#e74c3c' for s in speedups]\n",
    "    ax.bar(steps, speedups, color=colors, alpha=0.7, edgecolor='white')\n",
    "    ax.axhline(1.0, color='black', linestyle='--', linewidth=0.8)\n",
    "    ax.set_xlabel('Target step')\n",
    "    ax.set_ylabel('Speedup (Original / Fused)')\n",
    "    ax.set_title('Per-step speedup (>1 = Fused faster)')\n",
    "    ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "    logp_diffs = [abs(d0[i][2] - d1[i][2]) for i in range(n)]\n",
    "    print(f'Max |logp| diff: {max(logp_diffs):.6f}')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75fbae41-3a99-491e-8052-811b3b06a8ec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}