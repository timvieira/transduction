{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PTB Tokenizer Analysis\n",
    "\n",
    "This notebook explores the Penn Treebank (PTB) tokenizer FST and its quotient/remainder decomposition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transduction.benchmarking.fsts.ptb_pynini import (\n",
    "    build_ptb_fst_pynini,\n",
    "    string_to_byte_strs,\n",
    "    decode_ptb_output,\n",
    "    SEP,\n",
    "    MARKER,\n",
    ")\n",
    "from transduction.rust_bridge import RustDecomp\n",
    "from transduction.fsa import EPSILON\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "# Build the PTB FST\n",
    "print(\"Building PTB FST...\")\n",
    "fst = build_ptb_fst_pynini()\n",
    "print(f\"FST: {len(fst.states)} states\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Transduction Examples\n",
    "\n",
    "See how the PTB tokenizer transforms input text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transduce(text):\n",
    "    \"\"\"Transduce text through PTB FST and return detailed output.\"\"\"\n",
    "    byte_strs = string_to_byte_strs(text)\n",
    "    output_fsa = fst(byte_strs, None)\n",
    "    output = next(output_fsa.language(tuple=True))\n",
    "    return output\n",
    "\n",
    "def decode_with_markers(output, sep_char='|', marker_char=''):\n",
    "    \"\"\"Decode output showing token boundaries.\"\"\"\n",
    "    tokens = []\n",
    "    current = []\n",
    "    \n",
    "    for sym in output:\n",
    "        if sym == SEP:\n",
    "            if current:\n",
    "                tokens.append(bytes(int(b) for b in current).decode('utf-8', errors='replace'))\n",
    "                current = []\n",
    "            tokens.append(sep_char)\n",
    "        elif sym != MARKER and sym != EPSILON:\n",
    "            current.append(sym)\n",
    "    \n",
    "    if current:\n",
    "        tokens.append(bytes(int(b) for b in current).decode('utf-8', errors='replace'))\n",
    "    \n",
    "    return ''.join(tokens)\n",
    "\n",
    "def show_transduction(text):\n",
    "    \"\"\"Display transduction with formatting.\"\"\"\n",
    "    output = transduce(text)\n",
    "    decoded = decode_ptb_output(output)\n",
    "    with_boundaries = decode_with_markers(output)\n",
    "    \n",
    "    print(f\"Input:  {text}\")\n",
    "    print(f\"Output: {decoded}\")\n",
    "    print(f\"Tokens: {with_boundaries}\")\n",
    "    print(f\"        ({len(output)} symbols, {sum(1 for s in output if s == SEP) + 1} tokens)\")\n",
    "    print()\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test various inputs\n",
    "test_cases = [\n",
    "    \"Hello, world!\",\n",
    "    \"I can't believe it's not butter.\",\n",
    "    \"\\\"To be or not to be,\\\" he said.\",\n",
    "    \"The price is $19.99 (tax included).\",\n",
    "    \"Dr. Smith's patients aren't feeling well.\",\n",
    "    \"What?! That's impossible...\",\n",
    "]\n",
    "\n",
    "for text in test_cases:\n",
    "    show_transduction(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Interactive Transduction\n",
    "\n",
    "Enter your own text to see how it's tokenized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try your own text\n",
    "your_text = \"Enter your text here!\"\n",
    "\n",
    "show_transduction(your_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Quotient/Remainder Decomposition\n",
    "\n",
    "The decomposition splits inputs based on whether they have universal or constrained continuations:\n",
    "\n",
    "$$\\text{Precover}(y) = Q(y) \\cdot \\Sigma^* \\sqcup R(y)$$\n",
    "\n",
    "- **Q**: Inputs that can continue with ANY suffix (universal)\n",
    "- **R**: Inputs with constrained continuations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_decomposition(text, target_len):\n",
    "    \"\"\"Analyze Q/R decomposition for given text and target length.\"\"\"\n",
    "    byte_strs = string_to_byte_strs(text)\n",
    "    output = transduce(text)\n",
    "    \n",
    "    if target_len > len(output):\n",
    "        print(f\"Target length {target_len} exceeds output length {len(output)}\")\n",
    "        return None, None\n",
    "    \n",
    "    target = output[:target_len]\n",
    "    result = RustDecomp(fst, target)\n",
    "    Q = result.quotient\n",
    "    R = result.remainder\n",
    "    \n",
    "    # Decode target\n",
    "    target_decoded = decode_with_markers(target)\n",
    "    \n",
    "    print(f\"=== Decomposition Analysis ===\")\n",
    "    print(f\"Input text: {text}\")\n",
    "    print(f\"Full output: {decode_with_markers(output)}\")\n",
    "    print(f\"Target (first {target_len} symbols): {target_decoded}\")\n",
    "    print()\n",
    "    print(f\"Quotient Q: {len(Q.states)} states, {len(Q.stop)} final\")\n",
    "    print(f\"Remainder R: {len(R.states)} states, {len(R.stop)} final\")\n",
    "    \n",
    "    return Q, R, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze a specific example\n",
    "text = \"Hello, world!\"\n",
    "target_len = 8  # \"Hello\" + SEP + \",\" + SEP\n",
    "\n",
    "Q, R, target = analyze_decomposition(text, target_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_q_language(Q, max_words=20):\n",
    "    \"\"\"Show sample words accepted by Q.\"\"\"\n",
    "    print(f\"\\n=== Q Language (inputs producing exactly the target) ===\")\n",
    "    print(f\"Q has {len(Q.stop)} final states\")\n",
    "    print()\n",
    "    \n",
    "    count = 0\n",
    "    for word in Q.language(tuple=True):\n",
    "        try:\n",
    "            decoded = bytes(int(b) for b in word if int(b) < 256).decode('utf-8', errors='replace')\n",
    "            # Get FST output for this word\n",
    "            out = transduce(decoded)\n",
    "            out_decoded = decode_with_markers(out[:15]) + ('...' if len(out) > 15 else '')\n",
    "            print(f\"  {decoded!r:20} -> {out_decoded}\")\n",
    "        except Exception as e:\n",
    "            print(f\"  {word} (error: {e})\")\n",
    "        \n",
    "        count += 1\n",
    "        if count >= max_words:\n",
    "            print(f\"  ... ({count}+ words)\")\n",
    "            break\n",
    "\n",
    "show_q_language(Q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_r_language(R, max_words=20):\n",
    "    \"\"\"Show sample words accepted by R (constrained continuations).\"\"\"\n",
    "    print(f\"\\n=== R Language (constrained continuations) ===\")\n",
    "    print(f\"R has {len(R.stop)} final states\")\n",
    "    \n",
    "    if len(R.stop) == 0:\n",
    "        print(\"R is empty - all continuations are universal!\")\n",
    "        return\n",
    "    \n",
    "    print()\n",
    "    count = 0\n",
    "    for word in R.language(tuple=True):\n",
    "        try:\n",
    "            decoded = bytes(int(b) for b in word if int(b) < 256).decode('utf-8', errors='replace')\n",
    "            out = transduce(decoded)\n",
    "            out_decoded = decode_with_markers(out)\n",
    "            print(f\"  {decoded!r:20} -> {out_decoded}\")\n",
    "        except Exception as e:\n",
    "            print(f\"  {word} (error: {e})\")\n",
    "        \n",
    "        count += 1\n",
    "        if count >= max_words:\n",
    "            print(f\"  ... ({count}+ words)\")\n",
    "            break\n",
    "\n",
    "show_r_language(R)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Exploring Contraction Handling\n",
    "\n",
    "PTB has special rules for contractions which create non-universal (R) states."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Contractions create interesting Q/R splits\n",
    "text = \"I can't do it\"\n",
    "\n",
    "output = transduce(text)\n",
    "print(f\"Full transduction:\")\n",
    "show_transduction(text)\n",
    "\n",
    "# Analyze at different target lengths\n",
    "for tlen in [2, 4, 6, 8, 10]:\n",
    "    if tlen <= len(output):\n",
    "        Q, R, target = analyze_decomposition(text, tlen)\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at a case with non-empty R\n",
    "text = \"In 2006\"\n",
    "Q, R, target = analyze_decomposition(text, 2)  # Just \"In\"\n",
    "\n",
    "print(\"\\n--- Q accepts (universal continuations): ---\")\n",
    "show_q_language(Q, max_words=10)\n",
    "\n",
    "print(\"\\n--- R accepts (constrained continuations): ---\")\n",
    "show_r_language(R, max_words=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Visualize Q and R Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_automaton_structure(fsa, name, max_arcs=50):\n",
    "    \"\"\"Display automaton structure.\"\"\"\n",
    "    print(f\"\\n=== {name} Structure ===\")\n",
    "    print(f\"States: {len(fsa.states)}\")\n",
    "    print(f\"Start: {fsa.start}\")\n",
    "    print(f\"Final: {fsa.stop}\")\n",
    "    \n",
    "    arcs = list(fsa.arcs())\n",
    "    print(f\"Arcs: {len(arcs)} total\")\n",
    "    \n",
    "    if len(arcs) <= max_arcs:\n",
    "        print(\"\\nTransitions:\")\n",
    "        # Group by source state\n",
    "        from collections import defaultdict\n",
    "        by_src = defaultdict(list)\n",
    "        for src, lbl, dst in arcs:\n",
    "            by_src[src].append((lbl, dst))\n",
    "        \n",
    "        for src in sorted(by_src.keys()):\n",
    "            is_final = src in fsa.stop\n",
    "            is_start = src in fsa.start\n",
    "            markers = []\n",
    "            if is_start: markers.append('start')\n",
    "            if is_final: markers.append('final')\n",
    "            marker_str = f\" ({', '.join(markers)})\" if markers else \"\"\n",
    "            \n",
    "            trans = by_src[src]\n",
    "            trans_str = ', '.join(f\"{chr(int(l)) if int(l) < 128 else f'[{l}]'}:{d}\" for l, d in trans[:5])\n",
    "            if len(trans) > 5:\n",
    "                trans_str += f\", ... (+{len(trans)-5} more)\"\n",
    "            print(f\"  State {src}{marker_str}: {trans_str}\")\n",
    "    else:\n",
    "        print(f\"  (too many arcs to display)\")\n",
    "\n",
    "# Show Q structure for a small example\n",
    "text = \"Hi!\"\n",
    "Q, R, target = analyze_decomposition(text, 3)\n",
    "show_automaton_structure(Q, \"Quotient Q\")\n",
    "show_automaton_structure(R, \"Remainder R\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Benchmark Different Target Lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def benchmark_decomposition(text, step=5):\n",
    "    \"\"\"Benchmark decomposition at various target lengths.\"\"\"\n",
    "    output = transduce(text)\n",
    "    \n",
    "    print(f\"Text: {text[:50]}...\" if len(text) > 50 else f\"Text: {text}\")\n",
    "    print(f\"Output length: {len(output)} symbols\")\n",
    "    print()\n",
    "    print(f\"{'Target Len':<12} {'Q States':<12} {'Q Final':<10} {'R Final':<10} {'Time (ms)':<10}\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    results = []\n",
    "    for tlen in range(step, min(len(output), 100), step):\n",
    "        target = output[:tlen]\n",
    "        \n",
    "        t0 = time.perf_counter()\n",
    "        result = RustDecomp(fst, target)\n",
    "        t1 = time.perf_counter()\n",
    "        \n",
    "        Q = result.quotient\n",
    "        R = result.remainder\n",
    "        time_ms = (t1 - t0) * 1000\n",
    "        \n",
    "        print(f\"{tlen:<12} {len(Q.states):<12} {len(Q.stop):<10} {len(R.stop):<10} {time_ms:<10.1f}\")\n",
    "        results.append({\n",
    "            'target_len': tlen,\n",
    "            'q_states': len(Q.states),\n",
    "            'q_final': len(Q.stop),\n",
    "            'r_final': len(R.stop),\n",
    "            'time_ms': time_ms,\n",
    "        })\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Benchmark on a longer text\n",
    "long_text = \"The quick brown fox jumps over the lazy dog. This is a test sentence with various punctuation marks, contractions like don't and won't, and quoted text like \\\"hello world\\\".\"\n",
    "results = benchmark_decomposition(long_text, step=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Check Correctness Property\n",
    "\n",
    "Verify that `Precover(y) = Q(y) · Σ* ⊔ R(y)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def verify_correctness(text, target_len, test_inputs):\n",
    "    \"\"\"Verify the decomposition property for given test inputs.\"\"\"\n",
    "    output = transduce(text)\n",
    "    target = output[:target_len]\n",
    "    \n",
    "    result = RustDecomp(fst, target)\n",
    "    Q = result.quotient\n",
    "    R = result.remainder\n",
    "    \n",
    "    print(f\"Target: {decode_with_markers(target)}\")\n",
    "    print(f\"Q: {len(Q.stop)} final, R: {len(R.stop)} final\")\n",
    "    print()\n",
    "    \n",
    "    def has_q_prefix(byte_strs):\n",
    "        for i in range(1, len(byte_strs) + 1):\n",
    "            if byte_strs[:i] in Q:\n",
    "                return True\n",
    "        return False\n",
    "    \n",
    "    print(f\"{'Input':<20} {'Starts w/ target':<18} {'In Q·Σ*':<10} {'In R':<8} {'Correct':<8}\")\n",
    "    print(\"-\" * 70)\n",
    "    \n",
    "    all_correct = True\n",
    "    for inp in test_inputs:\n",
    "        byte_strs = string_to_byte_strs(inp)\n",
    "        try:\n",
    "            out = transduce(inp)\n",
    "            starts_with_target = out[:len(target)] == target\n",
    "        except:\n",
    "            starts_with_target = False\n",
    "        \n",
    "        in_q_sigma = has_q_prefix(byte_strs)\n",
    "        in_r = byte_strs in R\n",
    "        in_precover = in_q_sigma or in_r\n",
    "        \n",
    "        correct = (starts_with_target == in_precover)\n",
    "        if not correct:\n",
    "            all_correct = False\n",
    "        \n",
    "        status = \"OK\" if correct else \"FAIL\"\n",
    "        print(f\"{inp!r:<20} {str(starts_with_target):<18} {str(in_q_sigma):<10} {str(in_r):<8} {status:<8}\")\n",
    "    \n",
    "    print()\n",
    "    if all_correct:\n",
    "        print(\"All tests passed!\")\n",
    "    else:\n",
    "        print(\"Some tests FAILED!\")\n",
    "\n",
    "# Verify with contraction example\n",
    "verify_correctness(\n",
    "    \"In 2006\", \n",
    "    target_len=2, \n",
    "    test_inputs=[\"In\", \"In \", \"In'\", \"In't\", \"Inc\", \"Ink\", \"Index\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Load Wikitext Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transduction.benchmarking.data import load_wikitext, wikitext_detokenize\n",
    "\n",
    "# Load some real text\n",
    "dataset = load_wikitext(\"test\")\n",
    "\n",
    "# Get first few paragraphs\n",
    "paragraphs = []\n",
    "for item in dataset:\n",
    "    text = item[\"text\"].strip()\n",
    "    if text and not text.startswith(\"=\"):\n",
    "        detokenized = wikitext_detokenize(text)[:500]\n",
    "        try:\n",
    "            output = transduce(detokenized)\n",
    "            paragraphs.append((detokenized, output))\n",
    "        except:\n",
    "            continue\n",
    "        if len(paragraphs) >= 5:\n",
    "            break\n",
    "\n",
    "print(f\"Loaded {len(paragraphs)} paragraphs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display loaded paragraphs\n",
    "for i, (text, output) in enumerate(paragraphs):\n",
    "    print(f\"\\n=== Paragraph {i+1} ===\")\n",
    "    print(f\"Text ({len(text)} chars): {text[:100]}...\")\n",
    "    print(f\"Tokenized: {decode_with_markers(output)[:100]}...\")\n",
    "    print(f\"Output: {len(output)} symbols, {sum(1 for s in output if s == SEP)+1} tokens\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze decomposition for a real paragraph\n",
    "if paragraphs:\n",
    "    text, output = paragraphs[0]\n",
    "    print(f\"Analyzing: {text[:80]}...\")\n",
    "    print()\n",
    "    \n",
    "    for tlen in [5, 10, 20, 30, 50]:\n",
    "        if tlen < len(output):\n",
    "            Q, R, target = analyze_decomposition(text, tlen)\n",
    "            print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
