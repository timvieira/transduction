{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lazy Precover DFA: Bitset & Hash-Consing Optimizations\n",
    "\n",
    "This notebook walks through the Python port of the three key optimizations from the Rust `transduction-core` crate:\n",
    "\n",
    "1. **Integer packing** — Pack `(fst_state, buf_pos)` into a single `int`\n",
    "2. **Hash-consing (PowersetArena)** — Deduplicate DFA states, with a singleton fast path\n",
    "3. **Epsilon closure caching with productivity filtering** — Compute each closure once, keep only productive states\n",
    "\n",
    "These three optimizations combine in `LazyPrecoverDFA`, a drop-in replacement for `PrecoverNFA(fst, target).det()` that can be dramatically faster on BPE-style FSTs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transduction.lazy_precover_dfa import PowersetArena, PackedPrecoverNFA, LazyPrecoverDFA\n",
    "from transduction.precover_nfa import PrecoverNFA\n",
    "from transduction.fst import FST, EPSILON\n",
    "from transduction import examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Integer Packing\n",
    "\n",
    "The original `PrecoverNFA` uses tuple states `(fst_state, buffer_string)`:\n",
    "```python\n",
    "state = (0, ('x', 'a'))   # FST state 0, buffer has matched 'x','a'\n",
    "```\n",
    "\n",
    "The key insight: since the buffer is always a prefix of the target, we only need its **length**:\n",
    "```python\n",
    "packed = fst_state * stride + buf_pos   # single int!\n",
    "```\n",
    "\n",
    "This makes hashing O(1) instead of O(len(buffer))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target_len = 2\n",
      "stride     = 3  (= target_len + 1)\n",
      "num FST states = 5\n",
      "\n",
      "pack(2, 1)   = 7\n",
      "unpack(7)  = (2, 1)\n",
      "\n",
      "Tuple state:  (2, ('c',))   — hashes a 2-tuple of (int, tuple)\n",
      "Packed state: 7              — hashes a single int\n"
     ]
    }
   ],
   "source": [
    "fst = examples.samuel_example()   # FST with epsilon arcs\n",
    "target = ('c', 'x')\n",
    "\n",
    "nfa = PackedPrecoverNFA(fst, target)\n",
    "print(f\"target_len = {nfa.target_len}\")\n",
    "print(f\"stride     = {nfa.stride}  (= target_len + 1)\")\n",
    "print(f\"num FST states = {len(nfa._state_map)}\")\n",
    "print()\n",
    "\n",
    "# Pack and unpack a state\n",
    "packed = nfa.pack(2, 1)   # FST state 2, buf_pos 1\n",
    "print(f\"pack(2, 1)   = {packed}\")\n",
    "print(f\"unpack({packed})  = {nfa.unpack(packed)}\")\n",
    "print()\n",
    "\n",
    "# Compare with the tuple-based representation:\n",
    "print(\"Tuple state:  (2, ('c',))   — hashes a 2-tuple of (int, tuple)\")\n",
    "print(f\"Packed state: {packed}              — hashes a single int\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Hash-Consing with PowersetArena\n",
    "\n",
    "In powerset construction, DFA states are **sets of NFA states**. The original code uses `frozenset`, which:\n",
    "- Hashes the entire set on every lookup\n",
    "- Creates a new object for every distinct set\n",
    "- Is O(n) to hash where n = set size\n",
    "\n",
    "`PowersetArena` replaces this with **hash-consing**: each unique sorted tuple of NFA states is assigned an integer ID. Subsequent lookups return the same ID.\n",
    "\n",
    "### Singleton fast path\n",
    "\n",
    "For BPE FSTs, ~99% of powerset states are singletons (exactly one NFA state). The arena has a special `_single_map` that hashes just the bare int, avoiding tuple construction entirely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id0 = 0  (singleton {100})\n",
      "id1 = 1  (multi-element {200, 300})\n",
      "id2 = 0  (same singleton {100} → same ID!)\n",
      "\n",
      "is_final[id0] = True  (was False, updated to True on re-intern)\n",
      "is_final[id1] = True\n",
      "\n",
      "Singletons stored in _single_map: {100: 0}\n",
      "Multi-element stored in _map:      {(200, 300): 1}\n",
      "Total interned: 2\n"
     ]
    }
   ],
   "source": [
    "arena = PowersetArena()\n",
    "\n",
    "# Intern some NFA state sets\n",
    "id0 = arena.intern((100,), any_final=False)       # singleton → fast path\n",
    "id1 = arena.intern((200, 300), any_final=True)     # multi-element → general path\n",
    "id2 = arena.intern((100,), any_final=True)         # same set → SAME ID, finality updated\n",
    "\n",
    "print(f\"id0 = {id0}  (singleton {{100}})\")\n",
    "print(f\"id1 = {id1}  (multi-element {{200, 300}})\")\n",
    "print(f\"id2 = {id2}  (same singleton {{100}} → same ID!)\")\n",
    "print()\n",
    "print(f\"is_final[id0] = {arena.is_final[id0]}  (was False, updated to True on re-intern)\")\n",
    "print(f\"is_final[id1] = {arena.is_final[id1]}\")\n",
    "print()\n",
    "print(f\"Singletons stored in _single_map: {arena._single_map}\")\n",
    "print(f\"Multi-element stored in _map:      {arena._map}\")\n",
    "print(f\"Total interned: {len(arena)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why finality is updated on cache hits\n",
    "\n",
    "In incremental decomposition, the same NFA state set can be final at one target length but not another. By always overwriting `is_final` on cache hits, the arena stays correct when reused across steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Epsilon Closure Caching & Productivity Filtering\n",
    "\n",
    "For FSTs with epsilon (input) arcs, the powerset construction must epsilon-close each NFA state. This is done via BFS.\n",
    "\n",
    "### Productivity filtering\n",
    "\n",
    "Not all states reachable via epsilon arcs matter. A state is **productive** if:\n",
    "- Its FST state has at least one non-epsilon input arc (contributes DFA transitions), OR\n",
    "- It's NFA-final (affects powerset finality)\n",
    "\n",
    "**Transit-only** states (epsilon-input-only, non-final) are filtered out. For BPE FSTs where each token is a chain of epsilon arcs producing one byte each, this collapses the chain to just the endpoints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FST state productivity:\n",
      "  state 0: has_non_eps_input=True, is_final=True\n",
      "  state 1: has_non_eps_input=True, is_final=False\n",
      "  state 2: has_non_eps_input=True, is_final=True\n",
      "  state 3: has_non_eps_input=False, is_final=True\n",
      "  state 4: has_non_eps_input=True, is_final=True\n",
      "\n",
      "eps_closure of (0, buf=0):\n",
      "  (0, buf=0)  productive=True\n"
     ]
    }
   ],
   "source": [
    "fst = examples.samuel_example()\n",
    "target = ('c', 'x')\n",
    "nfa = PackedPrecoverNFA(fst, target)\n",
    "\n",
    "# Show which FST states are productive\n",
    "print(\"FST state productivity:\")\n",
    "for orig_state in sorted(fst.states):\n",
    "    int_s = nfa._state_map(orig_state)\n",
    "    has_neps = int_s in nfa._has_non_eps_input\n",
    "    is_fin = fst.is_final(orig_state)\n",
    "    print(f\"  state {orig_state}: has_non_eps_input={has_neps}, is_final={is_fin}\")\n",
    "\n",
    "print()\n",
    "\n",
    "# Demonstrate epsilon closure with filtering\n",
    "for s in nfa.start_states():\n",
    "    closure = nfa.eps_closure_single(s)\n",
    "    fst_state, buf_pos = nfa.unpack(s)\n",
    "    orig = nfa._inv_state_map[fst_state]\n",
    "    print(f\"eps_closure of ({orig}, buf={buf_pos}):\")\n",
    "    for cs in closure:\n",
    "        cs_fst, cs_buf = nfa.unpack(cs)\n",
    "        cs_orig = nfa._inv_state_map[cs_fst]\n",
    "        prod = nfa.is_productive(cs)\n",
    "        print(f\"  ({cs_orig}, buf={cs_buf})  productive={prod}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After first pass:  hits=0, misses=1\n",
      "After second pass: hits=1, misses=1  (no new misses!)\n"
     ]
    }
   ],
   "source": [
    "# Show cache hits vs misses\n",
    "nfa2 = PackedPrecoverNFA(fst, target)\n",
    "\n",
    "# First pass: all misses\n",
    "for s in nfa2.start_states():\n",
    "    nfa2.eps_closure_single(s)\n",
    "hits1, misses1 = nfa2.eps_cache_stats()\n",
    "print(f\"After first pass:  hits={hits1}, misses={misses1}\")\n",
    "\n",
    "# Second pass: all hits\n",
    "for s in nfa2.start_states():\n",
    "    nfa2.eps_closure_single(s)\n",
    "hits2, misses2 = nfa2.eps_cache_stats()\n",
    "print(f\"After second pass: hits={hits2}, misses={misses2}  (no new misses!)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. LazyPrecoverDFA: Putting It All Together\n",
    "\n",
    "The `LazyPrecoverDFA` wraps all three optimizations into a single `Lazy` automaton:\n",
    "\n",
    "```\n",
    "PackedPrecoverNFA  ─→  PowersetArena  ─→  LazyPrecoverDFA\n",
    "  (packed ints)         (hash-consing)      (Lazy interface)\n",
    "  (cached closures)     (singleton fast)    (arc caching)\n",
    "```\n",
    "\n",
    "It's a drop-in replacement for `PrecoverNFA(fst, target).det()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start state: 0 (is_final=False)\n",
      "Powerset size: 1 NFA states\n",
      "\n",
      "Arcs from start:\n",
      "  'a' → state 1  (final=False, powerset_size=2)\n",
      "\n",
      "DFA states created so far: 2\n"
     ]
    }
   ],
   "source": [
    "fst = examples.samuel_example()\n",
    "target = ('c', 'x')\n",
    "\n",
    "# Build the optimized DFA\n",
    "dfa = LazyPrecoverDFA(fst, target)\n",
    "\n",
    "# Explore from the start state\n",
    "[start] = list(dfa.start())\n",
    "print(f\"Start state: {start} (is_final={dfa.is_final(start)})\")\n",
    "print(f\"Powerset size: {dfa.powerset_size(start)} NFA states\")\n",
    "print()\n",
    "\n",
    "# Show arcs (lazily computed on first access)\n",
    "print(\"Arcs from start:\")\n",
    "for sym, dest in dfa.arcs(start):\n",
    "    print(f\"  {sym!r} → state {dest}  (final={dfa.is_final(dest)}, powerset_size={dfa.powerset_size(dest)})\")\n",
    "\n",
    "print(f\"\\nDFA states created so far: {dfa.num_states()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Materialized DFA: 3 states\n",
      "Stats after full expansion:\n",
      "  num_dfa_states: 3\n",
      "  num_expanded: 3\n",
      "  avg_powerset_size: 1.3333333333333333\n",
      "  max_powerset_size: 2\n",
      "  singleton_fraction: 0.6666666666666666\n",
      "  eps_cache_size: 5\n",
      "  eps_cache_hits: 2\n",
      "  eps_cache_misses: 5\n"
     ]
    }
   ],
   "source": [
    "# Materialize the full DFA to see all states\n",
    "materialized = dfa.materialize()\n",
    "print(f\"Materialized DFA: {len(materialized.states)} states\")\n",
    "print(f\"Stats after full expansion:\")\n",
    "for k, v in dfa.stats().items():\n",
    "    print(f\"  {k}: {v}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Inspecting DFA Internals\n",
    "\n",
    "Each DFA state is backed by a set of packed NFA states. We can unpack them to see what's inside."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DFA has 4 states\n",
      "\n",
      "DFA state 0  (1 NFA states):\n",
      "    (fst=0, buf_pos=0)\n",
      "  'a' → 1\n",
      "\n",
      "DFA state 1  (2 NFA states):\n",
      "    (fst=1, buf_pos=0)\n",
      "    (fst=2, buf_pos=1)\n",
      "  'a' → 2\n",
      "  'b' → 2\n",
      "\n",
      "DFA state 2  (1 NFA states):\n",
      "    (fst=4, buf_pos=2)\n",
      "  'a' → 3\n",
      "  'b' → 3\n",
      "\n",
      "DFA state 3 [FINAL]  (1 NFA states):\n",
      "    (fst=4, buf_pos=3)\n",
      "  'a' → 3\n",
      "  'b' → 3\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fst = examples.samuel_example()\n",
    "target = ('c', 'x', 'x')\n",
    "dfa = LazyPrecoverDFA(fst, target)\n",
    "dfa.materialize()   # expand all states\n",
    "\n",
    "print(f\"DFA has {dfa.num_states()} states\\n\")\n",
    "\n",
    "for sid in range(dfa.num_states()):\n",
    "    arcs = dfa.arcs(sid)\n",
    "    nfa_set = dfa.nfa_states(sid)\n",
    "    final_str = \" [FINAL]\" if dfa.is_final(sid) else \"\"\n",
    "    print(f\"DFA state {sid}{final_str}  ({len(nfa_set)} NFA states):\")\n",
    "    for packed in nfa_set:\n",
    "        int_s, buf_pos = dfa.unpack_nfa_state(packed)\n",
    "        orig = dfa._nfa._inv_state_map[int_s]\n",
    "        print(f\"    (fst={orig}, buf_pos={buf_pos})\")\n",
    "    if arcs:\n",
    "        for sym, dest in arcs:\n",
    "            print(f\"  {sym!r} → {dest}\")\n",
    "    else:\n",
    "        print(f\"  (no arcs)\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Equivalence with the Reference\n",
    "\n",
    "Let's verify that `LazyPrecoverDFA` accepts exactly the same language as `PrecoverNFA(...).det()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✓  small / target=('x',)                     |L| = 31\n",
      "  ✓  samuel / target=('c','x')                 |L| = 30\n",
      "  ✓  delete_b / target=('A',)                  |L| = 57\n",
      "  ✓  triplets / target=('a','a','a')           |L| = 1\n",
      "  ✓  lookahead / target=('x','a')              |L| = 14\n"
     ]
    }
   ],
   "source": [
    "from collections import deque\n",
    "\n",
    "def accepted_strings(dfa, alphabet, max_len=5):\n",
    "    \"\"\"Enumerate all accepted strings up to max_len.\"\"\"\n",
    "    accepted = set()\n",
    "    [start] = list(dfa.start())\n",
    "    worklist = deque([(start, ())])\n",
    "    visited = {(start, ())}\n",
    "    while worklist:\n",
    "        state, path = worklist.popleft()\n",
    "        if dfa.is_final(state):\n",
    "            accepted.add(path)\n",
    "        if len(path) >= max_len:\n",
    "            continue\n",
    "        for x in alphabet:\n",
    "            for dest in dfa.arcs_x(state, x):\n",
    "                key = (dest, path + (x,))\n",
    "                if key not in visited:\n",
    "                    visited.add(key)\n",
    "                    worklist.append((dest, path + (x,)))\n",
    "    return accepted\n",
    "\n",
    "\n",
    "test_cases = [\n",
    "    (examples.small(), ('x',), \"small / target=('x',)\"),\n",
    "    (examples.samuel_example(), ('c', 'x'), \"samuel / target=('c','x')\"),\n",
    "    (examples.delete_b(), ('A',), \"delete_b / target=('A',)\"),\n",
    "    (examples.triplets_of_doom(), ('a', 'a', 'a'), \"triplets / target=('a','a','a')\"),\n",
    "    (examples.lookahead(), ('x', 'a'), \"lookahead / target=('x','a')\"),\n",
    "]\n",
    "\n",
    "for fst, target, name in test_cases:\n",
    "    alpha = fst.A - {EPSILON}\n",
    "    ref = PrecoverNFA(fst, target).det()\n",
    "    opt = LazyPrecoverDFA(fst, target)\n",
    "    ref_s = accepted_strings(ref, alpha)\n",
    "    opt_s = accepted_strings(opt, alpha)\n",
    "    match = \"✓\" if ref_s == opt_s else \"✗\"\n",
    "    print(f\"  {match}  {name:40s}  |L| = {len(ref_s)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Performance on BPE-like FSTs\n",
    "\n",
    "The optimizations really shine on BPE-like FSTs, which have:\n",
    "- Long epsilon chains (one arc per byte in a token)\n",
    "- Many singleton powerset states (~99%)\n",
    "- Large state spaces that benefit from hash-consing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LazyPrecoverDFA: 0.0036s\n",
      "\n",
      "Stats:\n",
      "  num_dfa_states: 7\n",
      "  num_expanded: 7\n",
      "  avg_powerset_size: 11.571\n",
      "  max_powerset_size: 51\n",
      "  singleton_fraction: 0.000\n",
      "  eps_cache_size: 7\n",
      "  eps_cache_hits: 74\n",
      "  eps_cache_misses: 7\n",
      "\n",
      "Key takeaways:\n",
      "  - 0% of DFA states are singletons (fast path)\n",
      "  - Eps cache hit rate: 91% (74/81)\n",
      "  - 7 DFA states, avg powerset size 11.6\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "fst = examples.bpe_like(vocab_size=50, alphabet=tuple(\"abc\"), max_len=4)\n",
    "target = tuple(\"abcabc\")\n",
    "\n",
    "# Time the optimized version\n",
    "t0 = time.perf_counter()\n",
    "dfa = LazyPrecoverDFA(fst, target)\n",
    "dfa.materialize()\n",
    "t_opt = time.perf_counter() - t0\n",
    "\n",
    "print(f\"LazyPrecoverDFA: {t_opt:.4f}s\")\n",
    "print(f\"\\nStats:\")\n",
    "for k, v in dfa.stats().items():\n",
    "    if isinstance(v, float):\n",
    "        print(f\"  {k}: {v:.3f}\")\n",
    "    else:\n",
    "        print(f\"  {k}: {v}\")\n",
    "\n",
    "print(f\"\\nKey takeaways:\")\n",
    "stats = dfa.stats()\n",
    "print(f\"  - {stats['singleton_fraction']*100:.0f}% of DFA states are singletons (fast path)\")\n",
    "total = stats['eps_cache_hits'] + stats['eps_cache_misses']\n",
    "if total > 0:\n",
    "    print(f\"  - Eps cache hit rate: {stats['eps_cache_hits']/total*100:.0f}% ({stats['eps_cache_hits']}/{total})\")\n",
    "print(f\"  - {stats['num_dfa_states']} DFA states, avg powerset size {stats['avg_powerset_size']:.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. How It Maps to the Rust Code\n",
    "\n",
    "| Python | Rust | File |\n",
    "|--------|------|------|\n",
    "| `PowersetArena` | `PowersetArena` | `powerset.rs` |\n",
    "| `PowersetArena._single_map` | `single_map: FxHashMap<u64, u32>` | singleton fast path |\n",
    "| `PowersetArena._map` | `map: FxHashMap<Vec<u64>, u32>` | general path |\n",
    "| `PackedPrecoverNFA` | `PrecoverNFA` | `precover.rs` |\n",
    "| `pack(s, p) = s * stride + p` | `pack(s, p, tl) = s * (tl+1) + p` | same formula |\n",
    "| `eps_closure_single` | `eps_closure_single_cached` | BFS + filter + cache |\n",
    "| `is_productive` | `is_productive` | non-eps input OR final |\n",
    "| `compute_all_arcs` | `compute_all_arcs_into` | batch arc computation |\n",
    "| `LazyPrecoverDFA` | `LazyPrecoverDFA` | `lazy_precover.rs` |\n",
    "| `_ensure_arcs` | `ensure_arcs_for` | lazy expansion + caching |\n",
    "| `_arcs_buf` (reusable dict) | `arcs_buf: FxHashMap` | buffer reuse pattern |\n",
    "\n",
    "### Key difference\n",
    "\n",
    "In Rust, the NFA is re-created as a temporary for each DFA expansion step, and the epsilon cache is transferred in/out via `take_eps_cache()`. This is due to Rust's borrow checker. In Python, we just keep one NFA instance alive — same effect since the FST doesn't change."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
