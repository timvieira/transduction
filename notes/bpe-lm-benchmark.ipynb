{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1-autoreload",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2-header",
   "metadata": {},
   "source": [
    "# TransducedLM vs FusedTransducedLM Benchmark (BPE)\n",
    "\n",
    "Compares two approaches to computing next-byte log-probabilities through\n",
    "a BPE FST (maps GPT-2 token IDs â†’ bytes):\n",
    "\n",
    "- **TransducedLM**: two-phase (PeekabooState BFS decomposition, then LM-weighted search)\n",
    "- **FusedTransducedLM**: single-pass (interleaved decomposition + LM search, no separate BFS)\n",
    "\n",
    "Uses a 3-gram CharNgramLM over token IDs as the inner LM, with per-call\n",
    "timeouts and a process-wide memory limit.\n",
    "\n",
    "The BPE FST has a star topology: each token maps to its byte spelling via\n",
    "a chain of epsilon-input arcs, then a single token-consuming arc back to\n",
    "the start state. This creates a very different decomposition structure\n",
    "from the PTB tokenizer (which has complex rewrite rules and 296 states)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3-imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time, gc\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer\n",
    "from transduction.applications.bpe import bpe_wfst\n",
    "from transduction.fst import FST\n",
    "from transduction.fsa import EPSILON\n",
    "from transduction.lm.statelm import decode_hf_tokenizer\n",
    "from transduction.util import Timeout, timelimit, set_memory_limit\n",
    "set_memory_limit(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4-build-fst",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('gpt2', use_fast=False)\n",
    "\n",
    "# Collect token IDs used in training data to build a subsampled FST.\n",
    "# The full GPT-2 vocabulary has ~50k tokens; using all of them would make\n",
    "# the CharNgramLM alphabet (and thus logp_next) very large.  Subsampling\n",
    "# to tokens that actually appear keeps the benchmark fast.\n",
    "train_sentences = [\n",
    "    \"The quick brown fox jumps over the lazy dog.\",\n",
    "    \"A stitch in time saves nine.\",\n",
    "    \"To be or not to be, that is the question.\",\n",
    "    \"All that glitters is not gold.\",\n",
    "    \"Actions speak louder than words.\",\n",
    "    \"Practice makes perfect.\",\n",
    "    \"Where there is a will, there is a way.\",\n",
    "] * 3\n",
    "\n",
    "train_ids = [tokenizer.encode(s) for s in train_sentences]\n",
    "used_ids = sorted(set(tid for seq in train_ids for tid in seq))\n",
    "print(f'Unique token IDs in training data: {len(used_ids)}')\n",
    "\n",
    "# Build subsampled BPE FST from just the tokens we need\n",
    "_, _, _decode, _ = decode_hf_tokenizer(tokenizer)\n",
    "drop = {x.encode() for x in tokenizer.all_special_tokens}\n",
    "\n",
    "def subsampled_bpe_fst(decode, token_ids, drop=frozenset()):\n",
    "    \"\"\"Build a BPE FST from a subset of token IDs.\"\"\"\n",
    "    m = FST()\n",
    "    m.add_start(())\n",
    "    for i in token_ids:\n",
    "        x = decode[i]\n",
    "        if x in drop:\n",
    "            continue\n",
    "        bx = tuple(x)\n",
    "        for j in range(len(bx)):\n",
    "            m.add_arc(bx[:j], EPSILON, bytes([bx[j]]), bx[:j+1])\n",
    "        m.add_arc(bx, i, EPSILON, ())\n",
    "    m.add_stop(())\n",
    "    return m\n",
    "\n",
    "t0 = time.perf_counter()\n",
    "bpe_fst = subsampled_bpe_fst(_decode, used_ids, drop)\n",
    "print(f'BPE FST built in {time.perf_counter()-t0:.3f}s: '\n",
    "      f'{len(bpe_fst.states)} states, |A|={len(bpe_fst.A)}, |B|={len(bpe_fst.B)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5-target-lm",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate target byte sequence by transducing token IDs through the FST\n",
    "text = \"The quick brown fox jumps over the lazy dog.\"\n",
    "token_ids_test = tokenizer.encode(text)\n",
    "target_seq = list(bpe_fst.transduce(token_ids_test))\n",
    "print(f'Text: {text!r}')\n",
    "print(f'Token IDs: {token_ids_test}')\n",
    "print(f'Target (bytes): {len(target_seq)} symbols')\n",
    "print(f'Decoded: {bytes(target_seq).decode()!r}')\n",
    "\n",
    "# Train inner LM on token-ID sequences\n",
    "from transduction.lm.ngram import CharNgramLM\n",
    "source_alpha = bpe_fst.A - {EPSILON}\n",
    "inner_lm = CharNgramLM.train(train_ids, n=3, alpha=0.5, alphabet=source_alpha)\n",
    "print(f'Inner LM: alphabet={len(inner_lm.alphabet)} symbols')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6-bench-header",
   "metadata": {},
   "source": "## TransducedLM Scaling\n\nPer-step decode time for **TransducedLM** (two-phase: PeekabooState BFS\ndecomposition, then LM-weighted search) vs **FusedTransducedLM** (single-pass:\ninterleaved decomposition + LM search, no separate BFS).\n\nEach step includes both decomposition and LM search costs. Both use\n`K=20`, `max_expansions=200`, with a 3s timeout per step."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7-benchmark",
   "metadata": {},
   "outputs": [],
   "source": "from collections import defaultdict\nfrom transduction.lm.transduced import TransducedLM\nfrom transduction.lm.fused_transduced import FusedTransducedLM\n\nMAX_DECODE = 100             # number of decode steps\nMAX_SEARCH = 200             # max priority-queue steps per logp_next\nMAX_BEAM = 20                # max items carried forward\nLM_TIMEOUT = 3               # seconds per step\n\nlm_results = defaultdict(list)  # name -> [(step, time_s, logp)]\n\nfor name, cls in [\n    ('TransducedLM', TransducedLM),\n    ('FusedTransducedLM', FusedTransducedLM),\n]:\n    print(f'\\n{name} (K={MAX_BEAM}, max_expansions={MAX_SEARCH}):')\n    if cls is TransducedLM:\n        tlm = cls(inner_lm, bpe_fst, K=MAX_BEAM, max_expansions=MAX_SEARCH)\n    else:\n        tlm = cls(inner_lm, bpe_fst, max_steps=MAX_SEARCH, max_beam=MAX_BEAM)\n    try:\n        with timelimit(LM_TIMEOUT):\n            state = tlm.initial()\n    except (Timeout, MemoryError) as e:\n        print(f'  initial() failed: {type(e).__name__}: {e}')\n        continue\n    for i in range(min(MAX_DECODE, len(target_seq))):\n        y = target_seq[i]\n        try:\n            with timelimit(LM_TIMEOUT):\n                t0 = time.perf_counter()\n                lp = state.logp_next[y]\n                state = state >> y\n                t1 = time.perf_counter()\n        except Timeout:\n            print(f'  step {i+1} TIMEOUT ({LM_TIMEOUT}s)')\n            break\n        except MemoryError:\n            print(f'  step {i+1} OOM')\n            break\n        elapsed = t1 - t0\n        lm_results[name].append((i + 1, elapsed, lp))\n        print(f'  {i+1:2d}: {elapsed*1000:8.1f} ms  logp={lp:.4f}')\n    gc.collect()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8-summary",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary table\n",
    "print(f'\\n{\"Algorithm\":<25s} {\"Total (s)\":>10s} {\"Avg/step (ms)\":>14s} {\"Steps\":>6s}')\n",
    "print('-' * 57)\n",
    "for name, data in sorted(lm_results.items()):\n",
    "    total = sum(t for _, t, _ in data)\n",
    "    avg = total / len(data) * 1000\n",
    "    print(f'{name:<25s} {total:10.2f} {avg:14.1f} {len(data):6d}')\n",
    "if len(lm_results) == 2:\n",
    "    names = sorted(lm_results.keys())\n",
    "    d0, d1 = lm_results[names[0]], lm_results[names[1]]\n",
    "    t0 = sum(t for _, t, _ in d0)\n",
    "    t1 = sum(t for _, t, _ in d1)\n",
    "    if t1 > 0:\n",
    "        print(f'\\nFused speedup (overall): {t0/t1:.2f}x')\n",
    "    # Exclude step 1 (amortization penalty for Fused)\n",
    "    if len(d0) > 1 and len(d1) > 1:\n",
    "        t0_skip1 = sum(t for _, t, _ in d0[1:])\n",
    "        t1_skip1 = sum(t for _, t, _ in d1[1:])\n",
    "        if t1_skip1 > 0:\n",
    "            print(f'Fused speedup (step 2+): {t0_skip1/t1_skip1:.2f}x')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9-plot",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Left: time per step\n",
    "ax = axes[0]\n",
    "for name, data in sorted(lm_results.items()):\n",
    "    steps = [d[0] for d in data]\n",
    "    times = [d[1] * 1000 for d in data]  # ms\n",
    "    ax.plot(steps, times, 'o-', label=name, markersize=4)\n",
    "ax.set_xlabel('Target step (byte index)')\n",
    "ax.set_ylabel('Time per step (ms)')\n",
    "ax.set_title(f'TransducedLM vs Fused (BPE, max_steps={MAX_SEARCH})')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Right: per-step speedup\n",
    "ax = axes[1]\n",
    "if len(lm_results) == 2:\n",
    "    names = sorted(lm_results.keys())\n",
    "    d0, d1 = lm_results[names[0]], lm_results[names[1]]\n",
    "    n = min(len(d0), len(d1))\n",
    "    steps = [d0[i][0] for i in range(n)]\n",
    "    speedups = [d0[i][1] / d1[i][1] if d1[i][1] > 0 else 0 for i in range(n)]\n",
    "    colors = ['#2ecc71' if s > 1 else '#e74c3c' for s in speedups]\n",
    "    ax.bar(steps, speedups, color=colors, alpha=0.7, edgecolor='white')\n",
    "    ax.axhline(1.0, color='black', linestyle='--', linewidth=0.8)\n",
    "    ax.set_xlabel('Target step (byte index)')\n",
    "    ax.set_ylabel('Speedup (TransducedLM / Fused)')\n",
    "    ax.set_title('Per-step speedup (>1 = Fused faster)')\n",
    "    ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "    logp_diffs = [abs(d0[i][2] - d1[i][2]) for i in range(n)]\n",
    "    print(f'Max |logp| diff: {max(logp_diffs):.6f}')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a10-inspect-header",
   "metadata": {},
   "source": [
    "## FST Structure\n",
    "\n",
    "The BPE FST has a star topology: from the start state `()`, each token\n",
    "creates a chain of epsilon-input arcs that emit the token's bytes one at\n",
    "a time, then a single arc consuming the token ID returns to `()`.\n",
    "\n",
    "Below we show the FST statistics and a few example token chains."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a11-inspect",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show some example token chains\n",
    "sample_ids = token_ids_test[:8]\n",
    "for tid in sample_ids:\n",
    "    token_bytes = _decode[tid]\n",
    "    display_str = token_bytes.decode('utf-8', errors='replace')\n",
    "    print(f'  token {tid:5d} = {display_str!r:12s}  ({len(token_bytes)} bytes: {list(token_bytes)})')\n",
    "\n",
    "# Arc counts by type\n",
    "eps_in = sum(1 for s in bpe_fst.states for a, b, t in bpe_fst.arcs(s) if a == EPSILON)\n",
    "eps_out = sum(1 for s in bpe_fst.states for a, b, t in bpe_fst.arcs(s) if b == EPSILON)\n",
    "print(f'\\nFST: {len(bpe_fst.states)} states, eps-input arcs: {eps_in}, eps-output arcs: {eps_out}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a12-logp-inspect",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Most likely next bytes from the final state\n",
    "logp = state.logp_next\n",
    "syms = sorted(logp.keys(), key=lambda s: logp[s], reverse=True)[:15]\n",
    "print(f'{\"symbol\":>8s}  {\"char\":>6s}  {\"logp\":>8s}  {\"prob\":>8s}')\n",
    "print(\"-\" * 36)\n",
    "for s in syms:\n",
    "    if isinstance(s, (bytes, bytearray)):\n",
    "        val = s[0]\n",
    "        ch = chr(val) if 32 <= val <= 126 else f'\\\\x{val:02x}'\n",
    "    elif isinstance(s, int):\n",
    "        ch = chr(s) if 32 <= s <= 126 else f'\\\\x{s:02x}'\n",
    "    else:\n",
    "        ch = repr(s)\n",
    "    print(f'{str(s):>8s}  {ch:>6s}  {logp[s]:8.4f}  {np.exp(logp[s]):8.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a13-empty",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}