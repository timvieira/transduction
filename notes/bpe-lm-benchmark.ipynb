{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a1-autoreload",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2-header",
   "metadata": {},
   "source": [
    "# TransducedLM vs FusedTransducedLM Benchmark (BPE)\n",
    "\n",
    "Compares two approaches to computing next-byte log-probabilities through\n",
    "a BPE FST (maps GPT-2 token IDs → bytes):\n",
    "\n",
    "- **TransducedLM**: two-phase (PeekabooState BFS decomposition, then LM-weighted search)\n",
    "- **FusedTransducedLM**: single-pass (interleaved decomposition + LM search, no separate BFS)\n",
    "\n",
    "Uses a 3-gram CharNgramLM over token IDs as the inner LM, with per-call\n",
    "timeouts and a process-wide memory limit.\n",
    "\n",
    "The BPE FST has a star topology: each token maps to its byte spelling via\n",
    "a chain of epsilon-input arcs, then a single token-consuming arc back to\n",
    "the start state. This creates a very different decomposition structure\n",
    "from the PTB tokenizer (which has complex rewrite rules and 296 states)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a3-imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time, gc\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from transformers import AutoTokenizer\n",
    "from transduction.applications.bpe import bpe_wfst\n",
    "from transduction.fst import FST\n",
    "from transduction.fsa import EPSILON\n",
    "from transduction.lm.statelm import HfTokenizerVocab\n",
    "from transduction.util import Timeout, timelimit, set_memory_limit\n",
    "set_memory_limit(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "hs5pf9xk1ou",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Tunable parameters ----\n",
    "VOCAB_SIZE = 1_000    # None → only tokens from training data (~43)\n",
    "                       # int  → use that many from GPT-2 vocab (max ~50k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a4-build-fst",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training tokens: 43, Selected vocab: 1023 / 50256 total\n",
      "BPE FST built in 0.009s: 1313 states, |A|=1024, |B|=257\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('gpt2', use_fast=False, local_files_only=True)\n",
    "_decode = HfTokenizerVocab(tokenizer).decode\n",
    "drop = {x.encode() for x in tokenizer.all_special_tokens}\n",
    "\n",
    "# All non-special token IDs available\n",
    "all_token_ids = sorted(i for i in range(len(_decode)) if _decode[i] not in drop)\n",
    "\n",
    "# Training data for the inner n-gram LM\n",
    "train_sentences = [\n",
    "    \"The quick brown fox jumps over the lazy dog.\",\n",
    "    \"A stitch in time saves nine.\",\n",
    "    \"To be or not to be, that is the question.\",\n",
    "    \"All that glitters is not gold.\",\n",
    "    \"Actions speak louder than words.\",\n",
    "    \"Practice makes perfect.\",\n",
    "    \"Where there is a will, there is a way.\",\n",
    "] * 3\n",
    "\n",
    "train_ids = [tokenizer.encode(s) for s in train_sentences]\n",
    "train_used = sorted(set(tid for seq in train_ids for tid in seq))\n",
    "\n",
    "# Select vocabulary subset\n",
    "if VOCAB_SIZE is None:\n",
    "    used_ids = train_used\n",
    "else:\n",
    "    used_ids = all_token_ids[:VOCAB_SIZE]\n",
    "    # Always include training tokens so n-gram LM has valid data\n",
    "    used_ids = sorted(set(used_ids) | set(train_used))\n",
    "\n",
    "print(f'Training tokens: {len(train_used)}, Selected vocab: {len(used_ids)} / {len(all_token_ids)} total')\n",
    "\n",
    "# Build subsampled BPE FST\n",
    "def subsampled_bpe_fst(decode, token_ids, drop=frozenset()):\n",
    "    \"\"\"Build a BPE FST from a subset of token IDs.\"\"\"\n",
    "    m = FST()\n",
    "    m.add_start(())\n",
    "    for i in token_ids:\n",
    "        x = decode[i]\n",
    "        if x in drop:\n",
    "            continue\n",
    "        bx = tuple(x)\n",
    "        for j in range(len(bx)):\n",
    "            m.add_arc(bx[:j], EPSILON, bx[j], bx[:j+1])\n",
    "        m.add_arc(bx, i, EPSILON, ())\n",
    "    m.add_stop(())\n",
    "    return m\n",
    "\n",
    "t0 = time.perf_counter()\n",
    "bpe_fst = subsampled_bpe_fst(_decode, used_ids, drop)\n",
    "print(f'BPE FST built in {time.perf_counter()-t0:.3f}s: '\n",
    "      f'{len(bpe_fst.states)} states, |A|={len(bpe_fst.A)}, |B|={len(bpe_fst.B)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a5-target-lm",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: 'The quick brown fox jumps over the lazy dog.'\n",
      "Token IDs: [464, 2068, 7586, 21831, 18045, 625, 262, 16931, 3290, 13]\n",
      "Target (bytes): 44 symbols\n",
      "Decoded: 'The quick brown fox jumps over the lazy dog.'\n",
      "Inner LM: alphabet=1024 symbols\n"
     ]
    }
   ],
   "source": [
    "# Generate target byte sequence by transducing token IDs through the FST\n",
    "text = \"The quick brown fox jumps over the lazy dog.\"\n",
    "token_ids_test = tokenizer.encode(text)\n",
    "target_seq = list(bpe_fst.transduce(token_ids_test))\n",
    "print(f'Text: {text!r}')\n",
    "print(f'Token IDs: {token_ids_test}')\n",
    "print(f'Target (bytes): {len(target_seq)} symbols')\n",
    "print(f'Decoded: {bytes(target_seq).decode()!r}')\n",
    "\n",
    "# Train inner LM on token-ID sequences\n",
    "from transduction.lm.ngram import CharNgramLM\n",
    "source_alpha = bpe_fst.A - {EPSILON}\n",
    "inner_lm = CharNgramLM.train(train_ids, n=3, alpha=0.5, alphabet=source_alpha)\n",
    "print(f'Inner LM: alphabet={len(inner_lm.alphabet)} symbols')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6-bench-header",
   "metadata": {},
   "source": [
    "## TransducedLM Scaling\n",
    "\n",
    "Per-step decode time for **TransducedLM** (two-phase: PeekabooState BFS\n",
    "decomposition, then LM-weighted search) vs **FusedTransducedLM** (single-pass:\n",
    "interleaved decomposition + LM search, no separate BFS).\n",
    "\n",
    "Each step includes both decomposition and LM search costs. Both use\n",
    "`K=20`, `max_expansions=200`, with a 3s timeout per step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a7-benchmark",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "FusedTransducedLM (K=10, max_expansions=200):\n",
      "   1:     71.2 ms  logp=-4.4900\n",
      "   2:     68.5 ms  logp=-0.6925\n",
      "   3:     73.9 ms  logp=-0.1305\n",
      "   4:     70.1 ms  logp=-1.0085\n",
      "   5:     19.5 ms  logp=-0.8267\n",
      "   6:      0.7 ms  logp=0.0000\n",
      "   7:      0.4 ms  logp=0.0000\n",
      "   8:      0.3 ms  logp=0.0000\n",
      "   9:      0.3 ms  logp=0.0000\n",
      "  10:     58.1 ms  logp=-1.0077\n",
      "  11:     16.6 ms  logp=-0.8267\n",
      "  12:      0.9 ms  logp=0.0000\n",
      "  13:      0.4 ms  logp=0.0000\n",
      "  14:      0.2 ms  logp=0.0000\n",
      "  15:      0.3 ms  logp=0.0000\n",
      "  16:     67.8 ms  logp=-1.0077\n",
      "  17:     15.6 ms  logp=-0.6931\n",
      "  18:      1.1 ms  logp=-0.1335\n",
      "  19:      0.5 ms  logp=0.0000\n",
      "  20:     61.2 ms  logp=-1.0077\n",
      "  21:     16.8 ms  logp=-0.8267\n",
      "  22:      0.7 ms  logp=0.0000\n",
      "  23:      0.3 ms  logp=0.0000\n",
      "  24:      0.3 ms  logp=0.0000\n",
      "  25:      0.4 ms  logp=0.0000\n",
      "  26:     56.5 ms  logp=-1.0077\n",
      "  27:     15.8 ms  logp=-0.8267\n",
      "  28:      1.0 ms  logp=0.0000\n",
      "  29:      0.5 ms  logp=0.0000\n",
      "  30:      0.3 ms  logp=0.0000\n",
      "  31:     59.6 ms  logp=-1.0077\n",
      "  32:     19.5 ms  logp=-0.5754\n",
      "  33:      1.2 ms  logp=-0.2513\n",
      "  34:      0.7 ms  logp=0.0000\n",
      "  35:     64.7 ms  logp=-1.0077\n",
      "  36:     17.0 ms  logp=-0.8267\n",
      "  37:      0.9 ms  logp=0.0000\n",
      "  38:      0.4 ms  logp=0.0000\n",
      "  39:      0.3 ms  logp=0.0000\n",
      "  40:     61.8 ms  logp=-1.0077\n",
      "  41:     19.7 ms  logp=-0.8267\n",
      "  42:      1.2 ms  logp=0.0000\n",
      "  43:      0.6 ms  logp=0.0000\n",
      "  44:     61.5 ms  logp=-4.6347\n",
      "\n",
      "FusedTransducedLM (token) (K=10, max_expansions=200):\n",
      "   1:   3068.8 ms  logp=-4.4900\n",
      "   2:   3176.4 ms  logp=-0.6925\n",
      "   3:   3624.7 ms  logp=-0.1305\n",
      "   4:   4034.8 ms  logp=-1.0084\n",
      "   5:   5010.4 ms  logp=-0.8267\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 39\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m timelimit(LM_TIMEOUT):\n\u001b[1;32m     38\u001b[0m     t0 \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mperf_counter()\n\u001b[0;32m---> 39\u001b[0m     lp \u001b[38;5;241m=\u001b[39m \u001b[43mstate\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlogp_next\u001b[49m[y]\n\u001b[1;32m     40\u001b[0m     state \u001b[38;5;241m=\u001b[39m state \u001b[38;5;241m>>\u001b[39m y\n\u001b[1;32m     41\u001b[0m     t1 \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mperf_counter()\n",
      "File \u001b[0;32m~/projects/transduction/transduction/lm/fused_transduced.py:247\u001b[0m, in \u001b[0;36mFusedTransducedState.logp_next\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    245\u001b[0m \u001b[38;5;129m@property\u001b[39m\n\u001b[1;32m    246\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mlogp_next\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LogDistr[Token]:\n\u001b[0;32m--> 247\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_ensure_computed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    248\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_logp_next_cache\n",
      "File \u001b[0;32m~/projects/transduction/transduction/lm/fused_transduced.py:243\u001b[0m, in \u001b[0;36mFusedTransducedState._ensure_computed\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    241\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_ensure_computed\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    242\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_logp_next_cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 243\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_compute_logp_next\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/projects/transduction/transduction/lm/fused_transduced.py:266\u001b[0m, in \u001b[0;36mFusedTransducedState._compute_logp_next\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_compute_logp_next\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 266\u001b[0m     search \u001b[38;5;241m=\u001b[39m \u001b[43m_FusedSearch\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtlm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_target\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_particles\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    267\u001b[0m     scores, eos_score, carry_forward \u001b[38;5;241m=\u001b[39m search\u001b[38;5;241m.\u001b[39msearch()\n\u001b[1;32m    269\u001b[0m     scores[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39meos] \u001b[38;5;241m=\u001b[39m eos_score\n",
      "File \u001b[0;32m~/projects/transduction/transduction/lm/fused_transduced.py:69\u001b[0m, in \u001b[0;36m_FusedSearch.__init__\u001b[0;34m(self, tlm, target, particles)\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[38;5;66;03m# Create fresh Rust lazy DFA for this target prefix\u001b[39;00m\n\u001b[1;32m     68\u001b[0m target_u32 \u001b[38;5;241m=\u001b[39m [tlm\u001b[38;5;241m.\u001b[39m_sym_map[y] \u001b[38;5;28;01mfor\u001b[39;00m y \u001b[38;5;129;01min\u001b[39;00m target]\n\u001b[0;32m---> 69\u001b[0m \u001b[43mtlm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_rust_helper\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnew_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtarget_u32\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;66;03m# Search accumulators\u001b[39;00m\n\u001b[1;32m     72\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscores \u001b[38;5;241m=\u001b[39m LogVector()                     \u001b[38;5;66;03m# symbol -> log-weight\u001b[39;00m\n",
      "File \u001b[0;32m~/projects/transduction/transduction/token_decompose.py:391\u001b[0m, in \u001b[0;36mTokenPeekabooHelper.new_step\u001b[0;34m(self, target_u32)\u001b[0m\n\u001b[1;32m    389\u001b[0m arcs_list: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mtuple\u001b[39m[Any, \u001b[38;5;28mfrozenset\u001b[39m]] \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    390\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m x, dest_set \u001b[38;5;129;01min\u001b[39;00m by_label\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m--> 391\u001b[0m     dest_fs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_eps_closure\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnfa\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mfrozenset\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdest_set\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    392\u001b[0m     dest_key \u001b[38;5;241m=\u001b[39m _position_key(dest_fs, N)\n\u001b[1;32m    393\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m dest_key \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m canonical:\n",
      "File \u001b[0;32m~/projects/transduction/transduction/token_decompose.py:433\u001b[0m, in \u001b[0;36mTokenPeekabooHelper._eps_closure\u001b[0;34m(self, nfa, state_set)\u001b[0m\n\u001b[1;32m    431\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m (x, dest) \u001b[38;5;129;01min\u001b[39;00m nfa\u001b[38;5;241m.\u001b[39marcs(s):\n\u001b[1;32m    432\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m x \u001b[38;5;241m==\u001b[39m EPSILON \u001b[38;5;129;01mand\u001b[39;00m dest \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m result:\n\u001b[0;32m--> 433\u001b[0m             \u001b[43mresult\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    434\u001b[0m             worklist\u001b[38;5;241m.\u001b[39mappend(dest)\n\u001b[1;32m    435\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mfrozenset\u001b[39m(result)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "from transduction.lm.transduced import TransducedLM\n",
    "from transduction.lm.fused_transduced import FusedTransducedLM\n",
    "from transduction.token_decompose import TokenPeekabooState\n",
    "\n",
    "MAX_DECODE = 100             # number of decode steps\n",
    "MAX_SEARCH = 200             # max priority-queue steps per logp_next\n",
    "MAX_BEAM = 10                # max items carried forward\n",
    "LM_TIMEOUT = 10               # seconds per step\n",
    "\n",
    "lm_results = defaultdict(list)  # name -> [(step, time_s, logp)]\n",
    "\n",
    "configs = [\n",
    "#    ('TransducedLM', lambda: TransducedLM(\n",
    "#        inner_lm, bpe_fst, K=MAX_BEAM, max_expansions=MAX_SEARCH)),\n",
    "#    ('TransducedLM (token)', lambda: TransducedLM(\n",
    "#        inner_lm, bpe_fst, K=MAX_BEAM, max_expansions=MAX_SEARCH,\n",
    "#        decomp_state_cls=TokenPeekabooState, univ_cls=lambda fst: None)),\n",
    "    ('FusedTransducedLM', lambda: FusedTransducedLM(\n",
    "        inner_lm, bpe_fst, max_steps=MAX_SEARCH, max_beam=MAX_BEAM)),\n",
    "    ('FusedTransducedLM (token)', lambda: FusedTransducedLM(\n",
    "        inner_lm, bpe_fst, max_steps=MAX_SEARCH, max_beam=MAX_BEAM, helper='token')),\n",
    "]\n",
    "\n",
    "for name, make_tlm in configs:\n",
    "    print(f'\\n{name} (K={MAX_BEAM}, max_expansions={MAX_SEARCH}):')\n",
    "    try:\n",
    "        with timelimit(LM_TIMEOUT):\n",
    "            tlm = make_tlm()\n",
    "            state = tlm.initial()\n",
    "    except (Timeout, MemoryError, ValueError) as e:\n",
    "        print(f'  initial() failed: {type(e).__name__}: {e}')\n",
    "        continue\n",
    "    for i in range(min(MAX_DECODE, len(target_seq))):\n",
    "        y = target_seq[i]\n",
    "        try:\n",
    "            with timelimit(LM_TIMEOUT):\n",
    "                t0 = time.perf_counter()\n",
    "                lp = state.logp_next[y]\n",
    "                state = state >> y\n",
    "                t1 = time.perf_counter()\n",
    "        except Timeout:\n",
    "            print(f'  step {i+1} TIMEOUT ({LM_TIMEOUT}s)')\n",
    "            break\n",
    "        except MemoryError:\n",
    "            print(f'  step {i+1} OOM')\n",
    "            break\n",
    "        elapsed = t1 - t0\n",
    "        lm_results[name].append((i + 1, elapsed, lp))\n",
    "        print(f'  {i+1:2d}: {elapsed*1000:8.1f} ms  logp={lp:.4f}')\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8-summary",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary table\n",
    "print(f'\\n{\"Algorithm\":<25s} {\"Total (s)\":>10s} {\"Avg/step (ms)\":>14s} {\"Steps\":>6s}')\n",
    "print('-' * 57)\n",
    "for name, data in sorted(lm_results.items()):\n",
    "    total = sum(t for _, t, _ in data)\n",
    "    avg = total / len(data) * 1000\n",
    "    print(f'{name:<25s} {total:10.2f} {avg:14.1f} {len(data):6d}')\n",
    "\n",
    "# Pairwise logp agreement\n",
    "names = sorted(lm_results.keys())\n",
    "if len(names) >= 2:\n",
    "    ref_name = names[0]\n",
    "    ref_data = lm_results[ref_name]\n",
    "    print(f'\\nMax |logp| diff vs {ref_name}:')\n",
    "    for name in names[1:]:\n",
    "        data = lm_results[name]\n",
    "        n = min(len(ref_data), len(data))\n",
    "        if n > 0:\n",
    "            diffs = [abs(ref_data[i][2] - data[i][2]) for i in range(n)]\n",
    "            print(f'  {name}: {max(diffs):.6f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9-plot",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "\n",
    "for name, data in sorted(lm_results.items()):\n",
    "    steps = [d[0] for d in data]\n",
    "    times = [d[1] * 1000 for d in data]  # ms\n",
    "    ax.plot(steps, times, 'o-', label=name, markersize=4)\n",
    "ax.set_xlabel('Target step (byte index)')\n",
    "ax.set_ylabel('Time per step (ms)')\n",
    "ax.set_title(f'TransducedLM Variants (BPE, max_steps={MAX_SEARCH})')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ta5ueall4yr",
   "metadata": {},
   "source": [
    "## Vocab-Size Scaling\n",
    "\n",
    "How does FusedTransducedLM scale as the BPE vocabulary grows?\n",
    "At boundary DFA states, rho-arc compression stores a single default\n",
    "destination instead of |V| explicit arcs. But with Strategy A (full\n",
    "enumeration), we still push all |V| rho-class symbols into the priority\n",
    "queue at each expansion. This section measures the wall.\n",
    "\n",
    "The key metric is **avg ms/step at boundary steps** (where the target\n",
    "byte aligns with a token boundary, triggering expansion over all source\n",
    "symbols)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cef77b4-fc94-452d-9ce2-517ce6c7d05e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "V=43 (train)  FusedTransducedLM             avg=0ms  max=1ms  total=3ms  (8 steps)\n",
      "V=43 (train)  FusedTransducedLM (token)     avg=6ms  max=17ms  total=46ms  (8 steps)\n",
      "V=   240  FusedTransducedLM             avg=6ms  max=14ms  total=50ms  (8 steps)\n",
      "V=   240  FusedTransducedLM (token)     avg=150ms  max=306ms  total=1199ms  (8 steps)\n",
      "V=   529  FusedTransducedLM             avg=16ms  max=31ms  total=125ms  (8 steps)\n"
     ]
    }
   ],
   "source": [
    "import time, gc\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from transduction.fst import FST\n",
    "from transduction.fsa import EPSILON\n",
    "from transduction.lm.ngram import CharNgramLM\n",
    "from transduction.lm.transduced import TransducedLM\n",
    "from transduction.lm.fused_transduced import FusedTransducedLM\n",
    "from transduction.token_decompose import TokenPeekabooState\n",
    "from transduction.util import Timeout, timelimit\n",
    "\n",
    "text_short = \"The quick brown fox\"\n",
    "token_ids_short = tokenizer.encode(text_short)\n",
    "target_bytes_short = list(bpe_fst.transduce(token_ids_short))[:8]\n",
    "\n",
    "VOCAB_SIZES = [None, 200, 500, 1000]# 2000, 5000, 7000]\n",
    "SCALE_TIMEOUT = 60   # seconds per vocab size\n",
    "\n",
    "# ---- Methods to compare (name, factory(inner_lm, fst) -> TransducedLM) ----\n",
    "methods = [\n",
    "    ('FusedTransducedLM',       lambda lm, fst: FusedTransducedLM(lm, fst, max_steps=200, max_beam=10)),\n",
    "    ('FusedTransducedLM (token)', lambda lm, fst: FusedTransducedLM(lm, fst, max_steps=200, max_beam=10, helper='token')),\n",
    "    #('TransducedLM',            lambda lm, fst: TransducedLM(lm, fst, K=10, max_expansions=200)),\n",
    "    #('TransducedLM (token)',    lambda lm, fst: TransducedLM(lm, fst, K=10, max_expansions=200,\n",
    "    #                                decomp_state_cls=TokenPeekabooState, univ_cls=lambda fst: None)),\n",
    "]\n",
    "\n",
    "scale_results = defaultdict(list)  # method_name -> [(vocab_size, avg_ms, max_ms, total_ms, n_steps)]\n",
    "\n",
    "for vs in VOCAB_SIZES:\n",
    "    if vs is None:\n",
    "        used = train_used\n",
    "        label = f'{len(train_used)} (train)'\n",
    "    else:\n",
    "        used = sorted(set(all_token_ids[:vs]) | set(train_used))\n",
    "        label = str(len(used))\n",
    "\n",
    "    fst_v = subsampled_bpe_fst(_decode, used, drop)\n",
    "    source_alpha_v = fst_v.A - {EPSILON}\n",
    "    inner_v = CharNgramLM.train(train_ids, n=3, alpha=0.5, alphabet=source_alpha_v)\n",
    "\n",
    "    for method_name, make_tlm in methods:\n",
    "        tlm_v = make_tlm(inner_v, fst_v)\n",
    "        step_times = []\n",
    "        try:\n",
    "            with timelimit(SCALE_TIMEOUT):\n",
    "                state_v = tlm_v.initial()\n",
    "                for yb in target_bytes_short:\n",
    "                    t0 = time.perf_counter()\n",
    "                    _ = state_v.logp_next[yb]\n",
    "                    state_v = state_v >> yb\n",
    "                    step_times.append(time.perf_counter() - t0)\n",
    "        except (Timeout, MemoryError) as e:\n",
    "            print(f'V={label:>6s} {method_name}: {type(e).__name__} after {len(step_times)} steps')\n",
    "            if not step_times:\n",
    "                continue\n",
    "\n",
    "        avg = np.mean(step_times) * 1000\n",
    "        mx = np.max(step_times) * 1000\n",
    "        tot = np.sum(step_times) * 1000\n",
    "        scale_results[method_name].append((len(used), avg, mx, tot, len(step_times)))\n",
    "        print(f'V={label:>6s}  {method_name:<28s}  avg={avg:.0f}ms  max={mx:.0f}ms  total={tot:.0f}ms  ({len(step_times)} steps)')\n",
    "        gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0355e733-2546-440a-8e4c-3124b5ae04b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "for method_name, data in sorted(scale_results.items()):\n",
    "    vs_list = [r[0] for r in data]\n",
    "    avgs = [r[1] for r in data]\n",
    "    ax.plot(vs_list, avgs, 'o-', label=method_name, markersize=6)\n",
    "ax.set_xlabel('Vocabulary size |V|')\n",
    "ax.set_ylabel('Avg time per step (ms)')\n",
    "ax.set_title('BPE Vocab Scaling: Method Comparison')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "#ax.set_xscale('log')\n",
    "#ax.set_yscale('log')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cc00367-1df4-4c77-add8-e7edab809d66",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d0cdcd5-e8d0-4346-a487-61fc4960e848",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
