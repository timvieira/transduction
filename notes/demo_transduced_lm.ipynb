{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TransducedLM Demo\n",
    "\n",
    "This notebook demonstrates the `TransducedLM` — an incremental language model that computes the **pushforward** of an inner LM through an FST.\n",
    "\n",
    "Given:\n",
    "- An inner LM $P_{\\text{inner}}(\\text{source})$ over source strings\n",
    "- An FST mapping source → target\n",
    "\n",
    "TransducedLM computes $P(y \\mid \\text{target\\_so\\_far})$ by marginalizing over all source strings that produce the target prefix, using the peekaboo decomposition for incremental next-symbol computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "from transduction import examples, FST, EPSILON\n",
    "from transduction.lm.transduced import TransducedLM, logsumexp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A simple char-level n-gram LM\n",
    "\n",
    "We need an inner LM that implements the `StateLM` interface:\n",
    "- `lm.initial()` → initial state\n",
    "- `state << token` → advance by one token\n",
    "- `state.logp_next[token]` → log P(token | context)\n",
    "- `state.eos` → the EOS token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transduction.lm.ngram import CharNgramLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper: display a distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def copy_fst(alphabet):\n",
    "    \"\"\"Identity/copy transducer: maps each symbol to itself.\"\"\"\n",
    "    fst = FST()\n",
    "    fst.add_I(0)\n",
    "    fst.add_F(0)\n",
    "    for x in alphabet:\n",
    "        fst.add_arc(0, x, x, 0)\n",
    "    return fst\n",
    "\n",
    "\n",
    "def show_dist(state, symbols, show_zeros=False):\n",
    "    \"\"\"Display the full next-symbol distribution including EOS.\"\"\"\n",
    "    lp = state.logp_next\n",
    "    rows = []\n",
    "    all_logps = []\n",
    "    for y in sorted(symbols):\n",
    "        v = lp[y]\n",
    "        all_logps.append(v)\n",
    "        rows.append((repr(y), v, np.exp(v) if v > -50 else 0.0))\n",
    "    eos_v = lp[state.eos]\n",
    "    all_logps.append(eos_v)\n",
    "    rows.append(('EOS', eos_v, np.exp(eos_v) if eos_v > -50 else 0.0))\n",
    "\n",
    "    total = np.exp(logsumexp(all_logps))\n",
    "    print(f\"  target = {state._peekaboo_state.target!r}\")\n",
    "    for name, logp, prob in rows:\n",
    "        if not show_zeros and prob < 1e-6:\n",
    "            continue\n",
    "        bar = '#' * int(prob * 40)\n",
    "        print(f\"  P({name:>5s}) = {prob:.4f}  {bar}\")\n",
    "    print(f\"  {'sum':>10s} = {total:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Copy FST: TransducedLM reproduces the inner LM\n",
    "\n",
    "With an identity transducer (each symbol maps to itself), the transduced distribution should match the inner LM exactly — including EOS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From empty prefix:\n",
      "  target = ''\n",
      "  P(  'a') = 0.4969  ###################\n",
      "  P(  'b') = 0.4969  ###################\n",
      "  P(  EOS) = 0.0061  \n",
      "         sum = 1.000000\n",
      "\n",
      "Inner LM comparison:\n",
      "  'a': inner=-0.6993  transduced=-0.6993\n",
      "  'b': inner=-0.6993  transduced=-0.6993\n",
      "  EOS: inner=-5.0938  transduced=-5.0938\n"
     ]
    }
   ],
   "source": [
    "inner = CharNgramLM.train(\"aabbaabb\" * 10, n=2, alpha=0.5)\n",
    "fst = copy_fst([s for s in inner.alphabet if s != '<EOS>'])\n",
    "symbols = sorted(fst.B - {EPSILON})\n",
    "\n",
    "tlm = TransducedLM(inner, fst, max_steps=2000, max_beam=200)\n",
    "state = tlm.initial()\n",
    "\n",
    "print(\"From empty prefix:\")\n",
    "show_dist(state, symbols)\n",
    "\n",
    "print(\"\\nInner LM comparison:\")\n",
    "inner_state = inner.initial()\n",
    "for y in symbols:\n",
    "    print(f\"  {y!r}: inner={inner_state.logp_next[y]:.4f}  transduced={state.logp_next[y]:.4f}\")\n",
    "print(f\"  EOS: inner={inner_state.logp_next['<EOS>']:.4f}  transduced={state.logp_next[state.eos]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After advancing by 'a':\n",
      "  target = 'a'\n",
      "  P(  'a') = 0.4940  ###################\n",
      "  P(  'b') = 0.4940  ###################\n",
      "  P(  EOS) = 0.0120  \n",
      "         sum = 1.000000\n",
      "\n",
      "After advancing by 'a' then 'b':\n",
      "  target = 'ab'\n",
      "  P(  'a') = 0.4815  ###################\n",
      "  P(  'b') = 0.5062  ####################\n",
      "  P(  EOS) = 0.0123  \n",
      "         sum = 1.000000\n"
     ]
    }
   ],
   "source": [
    "print(\"After advancing by 'a':\")\n",
    "state_a = state << 'a'\n",
    "show_dist(state_a, symbols)\n",
    "\n",
    "print(\"\\nAfter advancing by 'a' then 'b':\")\n",
    "state_ab = state_a << 'b'\n",
    "show_dist(state_ab, symbols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Non-trivial FST: `examples.small()`\n",
    "\n",
    "The `small()` FST has structure:\n",
    "- State 0 (initial, final): `a→x→1`, `b→x→2`\n",
    "- State 1 (final): no outgoing arcs\n",
    "- State 2: `a→a→3`, `b→b→3`\n",
    "- State 3 (final): `a→a→3`, `b→b→3`\n",
    "\n",
    "So possible outputs are: `ε` (empty, from state 0), `x` (from source `a`, via state 1), and `x` followed by any string of `a`s and `b`s (from source `b...`, via states 2→3).\n",
    "\n",
    "Note that state 0 is final, so the empty source string produces the empty output — this gives non-trivial P(EOS) from the initial state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input alphabet:  ['a', 'b']\n",
      "Output alphabet: ['a', 'b', 'x']\n",
      "Initial: [0], Final: [0, 1, 3]\n",
      "\n",
      "Empty prefix — only 'x' and EOS are reachable:\n",
      "  target = ''\n",
      "  P(  'x') = 0.9839  #######################################\n",
      "  P(  EOS) = 0.0161  \n",
      "         sum = 1.000000\n"
     ]
    }
   ],
   "source": [
    "fst2 = examples.small()\n",
    "inner2 = CharNgramLM.train(\"aababba\" * 10, n=2, alpha=0.5)\n",
    "symbols2 = sorted(fst2.B - {EPSILON})\n",
    "\n",
    "print(f\"Input alphabet:  {sorted(fst2.A - {EPSILON})}\")\n",
    "print(f\"Output alphabet: {symbols2}\")\n",
    "print(f\"Initial: {sorted(fst2.I)}, Final: {sorted(fst2.F)}\")\n",
    "print()\n",
    "\n",
    "tlm2 = TransducedLM(inner2, fst2, max_steps=2000, max_beam=200)\n",
    "\n",
    "s = tlm2.initial()\n",
    "print(\"Empty prefix — only 'x' and EOS are reachable:\")\n",
    "show_dist(s, symbols2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After 'x':\n",
      "  Source 'a' maps to just 'x' (state 1, final, no outgoing arcs)\n",
      "  Source 'b...' maps to 'x' + continuation, but the LM puts\n",
      "  most mass on 'a', so almost all probability goes to EOS:\n",
      "  target = 'x'\n",
      "  P(  EOS) = 1.0000  ########################################\n",
      "         sum = 1.000000\n"
     ]
    }
   ],
   "source": [
    "s_x = s << 'x'\n",
    "print(\"After 'x':\")\n",
    "print(\"  Source 'a' maps to just 'x' (state 1, final, no outgoing arcs)\")\n",
    "print(\"  Source 'b...' maps to 'x' + continuation, but the LM puts\")\n",
    "print(\"  most mass on 'a', so almost all probability goes to EOS:\")\n",
    "show_dist(s_x, symbols2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "togglecase: [' ', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z'] → [' ', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n",
      "\n",
      "Empty prefix — uppercase symbols should dominate:\n",
      "  target = ''\n",
      "  P(  ' ') = 0.2726  ##########\n",
      "  P(  'A') = 0.3632  ##############\n",
      "  P(  'B') = 0.3632  ##############\n",
      "  P(  EOS) = 0.0009  \n",
      "         sum = 1.000000\n",
      "\n",
      "After 'A' (inner LM saw 'a' → next likely 'b' → output 'B'):\n",
      "  target = 'A'\n",
      "  P(  ' ') = 0.2563  ##########\n",
      "  P(  'A') = 0.2310  #########\n",
      "  P(  'B') = 0.5102  ####################\n",
      "  P(  EOS) = 0.0025  \n",
      "         sum = 1.000000\n",
      "\n",
      "After 'AB' (inner saw 'ab' → next likely ' ' → output ' '):\n",
      "  target = 'AB'\n",
      "  P(  ' ') = 0.4975  ###################\n",
      "  P(  'A') = 0.4975  ###################\n",
      "  P(  'B') = 0.0025  \n",
      "  P(  EOS) = 0.0025  \n",
      "         sum = 1.000000\n"
     ]
    }
   ],
   "source": [
    "# togglecase() maps a→A, b→B, A→a, B→b (and space→space)\n",
    "# With an inner LM trained on lowercase, the output will be uppercase.\n",
    "fst_tc = examples.togglecase()\n",
    "target_alpha_tc = sorted(fst_tc.B - {EPSILON})\n",
    "source_alpha_tc = sorted(fst_tc.A - {EPSILON})\n",
    "print(f\"togglecase: {source_alpha_tc} → {target_alpha_tc}\")\n",
    "\n",
    "inner_tc = CharNgramLM.train(\"ab ba ab ba\" * 10, n=2, alpha=0.1)\n",
    "tlm_tc = TransducedLM(inner_tc, fst_tc, max_steps=2000, max_beam=200)\n",
    "\n",
    "s = tlm_tc.initial()\n",
    "print(\"\\nEmpty prefix — uppercase symbols should dominate:\")\n",
    "show_dist(s, target_alpha_tc)\n",
    "\n",
    "s = s << 'A'\n",
    "print(\"\\nAfter 'A' (inner LM saw 'a' → next likely 'b' → output 'B'):\")\n",
    "show_dist(s, target_alpha_tc)\n",
    "\n",
    "s = s << 'B'\n",
    "print(\"\\nAfter 'AB' (inner saw 'ab' → next likely ' ' → output ' '):\")\n",
    "show_dist(s, target_alpha_tc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Autoregressive decoding\n",
    "\n",
    "We can use `TransducedLM` for greedy (or sampled) autoregressive decoding, stopping when EOS is the most likely next token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# greedy_decode and sample_decode are now methods on any LM state\n",
    "# via LMState — just call state.greedy_decode() or state.sample_decode().\n",
    "#\n",
    "# For verbose step-by-step output in this demo, we use a small wrapper:\n",
    "\n",
    "def show_greedy(state, max_len=15):\n",
    "    \"\"\"Greedy decode with per-step printing.\"\"\"\n",
    "    output = []\n",
    "    for step in range(max_len):\n",
    "        lp = state.logp_next\n",
    "        best_tok = lp.argmax()\n",
    "        best_lp = lp[best_tok]\n",
    "        eos_lp = lp[state.eos]\n",
    "        if best_tok == state.eos:\n",
    "            print(f\"  step {step}: EOS (logp={eos_lp:.3f})\")\n",
    "            break\n",
    "        output.append(best_tok)\n",
    "        print(f\"  step {step}: {best_tok!r}  (logp={best_lp:.3f}, P(EOS)={np.exp(eos_lp):.4f})\")\n",
    "        state = state << best_tok\n",
    "    return output, state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Greedy decoding with copy FST (inner LM trained on 'abababab'):\n",
      "  step 0: 'a'  (logp=-0.694, P(EOS)=0.0006)\n",
      "  step 1: 'b'  (logp=-0.002, P(EOS)=0.0012)\n",
      "  step 2: 'a'  (logp=-0.003, P(EOS)=0.0013)\n",
      "  step 3: 'b'  (logp=-0.002, P(EOS)=0.0012)\n",
      "  step 4: 'a'  (logp=-0.003, P(EOS)=0.0013)\n",
      "  step 5: 'b'  (logp=-0.002, P(EOS)=0.0012)\n",
      "  step 6: 'a'  (logp=-0.003, P(EOS)=0.0013)\n",
      "  step 7: 'b'  (logp=-0.002, P(EOS)=0.0012)\n",
      "  step 8: 'a'  (logp=-0.003, P(EOS)=0.0013)\n",
      "  step 9: 'b'  (logp=-0.002, P(EOS)=0.0012)\n",
      "  step 10: 'a'  (logp=-0.003, P(EOS)=0.0013)\n",
      "  step 11: 'b'  (logp=-0.002, P(EOS)=0.0012)\n",
      "  step 12: 'a'  (logp=-0.003, P(EOS)=0.0013)\n",
      "  step 13: 'b'  (logp=-0.002, P(EOS)=0.0012)\n",
      "  step 14: 'a'  (logp=-0.003, P(EOS)=0.0013)\n",
      "  => 'abababababababa'  (logp=-0.7289)\n",
      "\n",
      "  state.greedy_decode() => ['a', 'b', 'a', 'b', 'a', 'b', 'a', 'b', 'a', 'b', 'a', 'b', 'a', 'b', 'a']\n"
     ]
    }
   ],
   "source": [
    "inner3 = CharNgramLM.train(\"abababab\" * 20, n=2, alpha=0.1)\n",
    "fst3 = copy_fst(['a', 'b'])\n",
    "symbols3 = sorted(fst3.B - {EPSILON})\n",
    "\n",
    "tlm3 = TransducedLM(inner3, fst3, max_steps=2000, max_beam=200)\n",
    "\n",
    "print(\"Greedy decoding with copy FST (inner LM trained on 'abababab'):\")\n",
    "decoded, final_state = show_greedy(tlm3.initial())\n",
    "print(f\"  => {''.join(str(t) for t in decoded)!r}  (logp={final_state.logp:.4f})\")\n",
    "\n",
    "# Equivalently, without verbose output:\n",
    "tokens = tlm3.initial().greedy_decode(max_len=15)\n",
    "print(f\"\\n  state.greedy_decode() => {tokens!r}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampling (5 draws) via state.sample_decode():\n",
      "  0: 'abababababababa'\n",
      "  1: 'abababababababa'\n",
      "  2: 'abababababababa'\n",
      "  3: 'abababababbabab'\n",
      "  4: 'abababababababa'\n"
     ]
    }
   ],
   "source": [
    "print(\"Sampling (5 draws) via state.sample_decode():\")\n",
    "for i in range(5):\n",
    "    tokens = tlm3.initial().sample_decode(max_len=15)\n",
    "    print(f\"  {i}: {''.join(str(t) for t in tokens)!r}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Decoding through a non-trivial FST\n",
    "\n",
    "Decode through `togglecase()` — the inner LM is trained on lowercase text, but the output is uppercase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Greedy decoding through togglecase() FST:\n",
      "  step 0: 'A'  (logp=-1.030, P(EOS)=0.0007)\n",
      "  step 1: 'B'  (logp=-0.515, P(EOS)=0.0020)\n",
      "  step 2: 'A'  (logp=-0.529, P(EOS)=0.0020)\n",
      "  step 3: 'B'  (logp=-0.515, P(EOS)=0.0020)\n",
      "  step 4: 'A'  (logp=-0.529, P(EOS)=0.0020)\n",
      "  step 5: 'B'  (logp=-0.515, P(EOS)=0.0020)\n",
      "  step 6: 'A'  (logp=-0.529, P(EOS)=0.0020)\n",
      "  step 7: 'B'  (logp=-0.515, P(EOS)=0.0020)\n",
      "  step 8: 'A'  (logp=-0.529, P(EOS)=0.0020)\n",
      "  step 9: 'B'  (logp=-0.515, P(EOS)=0.0020)\n",
      "  step 10: 'A'  (logp=-0.529, P(EOS)=0.0020)\n",
      "  step 11: 'B'  (logp=-0.515, P(EOS)=0.0020)\n",
      "  step 12: 'A'  (logp=-0.529, P(EOS)=0.0020)\n",
      "  step 13: 'B'  (logp=-0.515, P(EOS)=0.0020)\n",
      "  step 14: 'A'  (logp=-0.529, P(EOS)=0.0020)\n",
      "  => 'ABABABABABABABA'\n",
      "\n",
      "Sampling (5 draws) via state.sample_decode():\n",
      "  0: 'B BABAB A B A A'\n",
      "  1: 'ABAB A A B A AB'\n",
      "  2: 'BA A BA ABAB A '\n",
      "  3: ' BAB BA BA B BA'\n",
      "  4: ' ABABAB B ABAB '\n"
     ]
    }
   ],
   "source": [
    "inner4 = CharNgramLM.train(\"ab ba ab ba ab\" * 10, n=2, alpha=0.1)\n",
    "fst4 = examples.togglecase()\n",
    "symbols4 = sorted(fst4.B - {EPSILON})\n",
    "\n",
    "tlm4 = TransducedLM(inner4, fst4, max_steps=2000, max_beam=200)\n",
    "\n",
    "print(\"Greedy decoding through togglecase() FST:\")\n",
    "decoded, _ = show_greedy(tlm4.initial())\n",
    "print(f\"  => {''.join(str(t) for t in decoded)!r}\")\n",
    "\n",
    "print(\"\\nSampling (5 draws) via state.sample_decode():\")\n",
    "for i in range(5):\n",
    "    tokens = tlm4.initial().sample_decode(max_len=15)\n",
    "    print(f\"  {i}: {''.join(str(t) for t in tokens)!r}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Normalization check\n",
    "\n",
    "Verify that at each step, the distribution over symbols + EOS sums to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  target=    ''  sum=1.0000000000  P(EOS)=0.006135\n",
      "  target=   'a'  sum=1.0000000000  P(EOS)=0.012048\n",
      "  target=  'ab'  sum=1.0000000000  P(EOS)=0.012346\n",
      "  target= 'aba'  sum=1.0000000000  P(EOS)=0.012048\n",
      "  target='abab'  sum=1.0000000000  P(EOS)=0.012346\n"
     ]
    }
   ],
   "source": [
    "inner5 = CharNgramLM.train(\"aabbaabb\" * 10, n=2, alpha=0.5)\n",
    "fst5 = copy_fst([s for s in inner5.alphabet if s != '<EOS>'])\n",
    "symbols5 = sorted(fst5.B - {EPSILON})\n",
    "\n",
    "tlm5 = TransducedLM(inner5, fst5, max_steps=2000, max_beam=200)\n",
    "state = tlm5.initial()\n",
    "\n",
    "for target_sym in ['a', 'b', 'a', 'b']:\n",
    "    lp = state.logp_next\n",
    "    all_logps = [lp[y] for y in symbols5] + [lp[state.eos]]\n",
    "    total = np.exp(logsumexp(all_logps))\n",
    "    print(f\"  target={state._peekaboo_state.target!r:>6s}  \"\n",
    "          f\"sum={total:.10f}  \"\n",
    "          f\"P(EOS)={np.exp(lp[state.eos]):.6f}\")\n",
    "    state = state << target_sym\n",
    "\n",
    "# Final state\n",
    "lp = state.logp_next\n",
    "all_logps = [lp[y] for y in symbols5] + [lp[state.eos]]\n",
    "total = np.exp(logsumexp(all_logps))\n",
    "print(f\"  target={state._peekaboo_state.target!r:>6s}  \"\n",
    "      f\"sum={total:.10f}  \"\n",
    "      f\"P(EOS)={np.exp(lp[state.eos]):.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
