{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Byte N-gram LM Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys; sys.path.insert(0, '..')\n",
    "import numpy as np\n",
    "from transduction.lm.ngram import ByteNgramLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train on some text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = b\"\"\"\n",
    "the cat sat on the mat. the dog sat on the log.\n",
    "the cat chased the dog. the dog chased the cat.\n",
    "a bird flew over the lazy dog. the quick brown fox jumped.\n",
    "the cat is on the mat. the dog is on the log.\n",
    "\"\"\" * 10  # repeat for more counts\n",
    "\n",
    "lm = ByteNgramLM.train(corpus, n=4, alpha=0.01)\n",
    "print(lm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = lm.initial()\n",
    "\n",
    "# Advance one byte at a time\n",
    "for ch in b'the ':\n",
    "    state = state >> bytes([ch])\n",
    "\n",
    "print(state)\n",
    "print(f'Cumulative logp: {state.logp:.3f}')\n",
    "print()\n",
    "\n",
    "# Top predictions\n",
    "top = state.logp_next.materialize(top=10)\n",
    "for tok, lp in top.items():\n",
    "    print(f'  {tok!r:6s}  logp={lp:+.2f}  p={np.exp(lp):.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Greedy decoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for prompt in [b'the ', b'the c', b'a ', b'the d']:\n",
    "    print(f'{prompt!r:10s} -> {b\"\".join(lm.initial()(prompt).greedy_decode())!r}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b''.join(lm.initial()(b'the ').sample_decode())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Score a string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score(lm, text):\n",
    "    if isinstance(text, str): text = text.encode()\n",
    "    state = lm.initial()\n",
    "    for ch in text:\n",
    "        state = state >> bytes([ch])\n",
    "    return state.logp\n",
    "\n",
    "texts = [\n",
    "    b'the cat sat on the mat.',\n",
    "    b'the dog sat on the log.',\n",
    "    b'the cat sat on the log.',   # mixed\n",
    "    b'xyzzy plugh grault.',       # nonsense\n",
    "]\n",
    "\n",
    "for t in texts:\n",
    "    lp = score(lm, t)\n",
    "    print(f'  logp={lp:+8.2f}  ppl={np.exp(-lp/len(t)):8.1f}  {t!r}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train on WikiText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from transduction.applications.wikitext import load_wikitext\n\n# Collect training text\nchunks = []\nfor item in load_wikitext('train'):\n    text = item['text'].strip()\n    if text:\n        chunks.append(text.encode('utf-8'))\n    if sum(len(c) for c in chunks) > 1_000_000:\n        break\n\ntrain_data = b'\\n'.join(chunks)\nprint(f'Training data: {len(train_data):,} bytes')\n\nlm_wiki = ByteNgramLM.train(train_data, n=5, alpha=0.001)\nprint(lm_wiki)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def advance(lm, prompt):\n    state = lm.initial()\n    for ch in prompt:\n        state = state >> bytes([ch])\n    return state\n\nfor prompt in [b'The ', b'In 200', b'He was ']:\n    decoded = b''.join(advance(lm_wiki, prompt).greedy_decode(max_len=80))\n    print(f'{prompt!r:12s} -> {decoded!r}')\nprint()\nfor _ in range(5):\n    sampled = b''.join(advance(lm_wiki, b'The ').sample_decode(max_len=80))\n    print(f'  {sampled!r}')"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prioritized enumeration: lowercase FST\n",
    "\n",
    "Given a target lowercase string, find the most likely mixed-case input under the WikiText n-gram LM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from transduction.fst import FST\nfrom transduction.rust_bridge import RustDecomp\nfrom transduction.enumeration import prioritized_enumeration\n\n# Byte-level lowercase FST: maps uppercase ASCII bytes to lowercase,\n# passes lowercase and space through unchanged.\n# Uses bytes labels to match ByteNgramLM's vocabulary.\ndef byte_lowercase_fst():\n    fst = FST()\n    fst.add_start(0)\n    fst.add_stop(0)\n    for i in range(256):\n        c = chr(i) if i < 128 else None\n        if c and c.isalpha():\n            lo = ord(c.lower())\n            fst.add_arc(0, bytes([i]), bytes([lo]), 0)\n        elif c == ' ':\n            fst.add_arc(0, bytes([i]), bytes([i]), 0)\n    return fst\n\nfst = byte_lowercase_fst()\nprint(f'Lowercase FST: {len(fst.states)} state, {len(fst.A)} input symbols, {len(fst.B)} output symbols')"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Target: \"in january\" — appears in WikiText as \"in January\"\ntarget_str = b'in january'\ntarget = tuple(bytes([b]) for b in target_str)\n\n# Decompose\nresult = RustDecomp(fst, target)\nQ, R = result.quotient, result.remainder\nprint(f'Target: {target_str!r}')\nprint(f'Q: {len(Q.states)} states, {len(Q.stop)} final')\nprint(f'R: {len(R.states)} states, {len(R.stop)} final')\n\n# Enumerate: find the most likely mixed-case inputs that lowercase to target_str\npe = prioritized_enumeration(lm_wiki.initial(), fst, target, max_steps=100000, \n                             decompose=RustDecomp)\n\nprint(f'\\nFound {len(pe.quotient_terms)} quotient, {len(pe.remainder_terms)} remainder')\nprint(f'\\nMost likely inputs that lowercase to {target_str!r}:')\nall_terms = sorted(pe.quotient_terms + pe.remainder_terms, key=lambda x: -x.weight)\nfor item in all_terms[:10]:\n    kind = 'Q' if item in pe.quotient_terms else 'R'\n    input_bytes = item.source.path_bytes()\n    print(f'  [{kind}] logp={item.weight:+.3f}  {input_bytes}')"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "Q.min().graphviz(fmt_edge=lambda i,a,j: a.decode('latin-1').replace(' ', '␣'))"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}