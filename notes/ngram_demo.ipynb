{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Byte N-gram LM Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from transduction.lm.ngram import ByteNgramLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train on some text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ByteNgramLM(n=4, contexts=201)\n"
     ]
    }
   ],
   "source": [
    "corpus = b\"\"\"\n",
    "the cat sat on the mat. the dog sat on the log.\n",
    "the cat chased the dog. the dog chased the cat.\n",
    "a bird flew over the lazy dog. the quick brown fox jumped.\n",
    "the cat is on the mat. the dog is on the log.\n",
    "\"\"\" * 10  # repeat for more counts\n",
    "\n",
    "lm = ByteNgramLM.train(corpus, n=4, alpha=0.01)\n",
    "print(lm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "state = lm.initial()\n\n# Advance one byte at a time\nfor ch in b'the ':\n    state = state >> ch\n\nprint(state)\nprint(f'Cumulative logp: {state.logp:.3f}')\nprint()\n\n# Top predictions\ntop = state.logp_next.materialize(top=10)\nfor tok, lp in top.items():\n    print(f'  {tok!r:6s}  logp={lp:+.2f}  p={np.exp(lp):.3f}')"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "bytes(lm(b'the ').sample_decode())"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Score a string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def score(lm, text):\n    if isinstance(text, str): text = text.encode()\n    state = lm(text)\n    return state.logp\n\ntexts = [\n    b'the cat sat on the mat.',\n    b'the dog sat on the log.',\n    b'the cat sat on the log.',   # mixed\n    b'xyzzy plugh grault.',       # nonsense\n]\n\nfor t in texts:\n    lp = score(lm, t)\n    print(f'  logp={lp:+8.2f}  ppl={np.exp(-lp/len(t)):8.1f}  {t!r}')"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train on WikiText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data: 1,002,885 bytes\n",
      "ByteNgramLM(n=5, contexts=71699)\n"
     ]
    }
   ],
   "source": [
    "from transduction.applications.wikitext import load_wikitext\n",
    "\n",
    "# Collect training text\n",
    "chunks = []\n",
    "for item in load_wikitext('train'):\n",
    "    text = item['text'].strip()\n",
    "    if text:\n",
    "        chunks.append(text.encode('utf-8'))\n",
    "    if sum(len(c) for c in chunks) > 1_000_000:\n",
    "        break\n",
    "\n",
    "train_data = b'\\n'.join(chunks)\n",
    "print(f'Training data: {len(train_data):,} bytes')\n",
    "\n",
    "lm_wiki = ByteNgramLM.train(train_data, n=5, alpha=0.001)\n",
    "print(lm_wiki)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "for prompt in [b'The ', b'In 200', b'He was ']:\n    decoded = bytes(lm_wiki(prompt).greedy_decode(max_len=80))\n    print(f'{prompt!r:12s} -> {decoded!r}')\nprint()\nfor _ in range(5):\n    sampled = bytes(lm_wiki(b'The ').sample_decode(max_len=80))\n    print(f'  {sampled!r}')"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prioritized enumeration: lowercase FST\n",
    "\n",
    "Given a target lowercase string, find the most likely mixed-case input under the WikiText n-gram LM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from transduction.fst import FST\nfrom transduction.rust_bridge import RustDecomp\nfrom transduction.enumeration import prioritized_enumeration\n\n# Byte-level lowercase FST: maps uppercase ASCII bytes to lowercase,\n# passes lowercase and space through unchanged.\n# Uses int labels (0-255) to match ByteNgramLM's vocabulary.\ndef byte_lowercase_fst():\n    fst = FST()\n    fst.add_start(0)\n    fst.add_stop(0)\n    for i in range(256):\n        c = chr(i) if i < 128 else None\n        if c and c.isalpha():\n            lo = ord(c.lower())\n            fst.add_arc(0, i, lo, 0)\n        elif c == ' ':\n            fst.add_arc(0, i, i, 0)\n    return fst\n\nfst = byte_lowercase_fst()\nprint(f'Lowercase FST: {len(fst.states)} state, {len(fst.A)} input symbols, {len(fst.B)} output symbols')"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Target: \"in january\" — appears in WikiText as \"in January\"\ntarget_str = b'in january'\ntarget = tuple(target_str)\n\n# Decompose\nresult = RustDecomp(fst, target)\nQ, R = result.quotient, result.remainder\nprint(f'Target: {target_str!r}')\nprint(f'Q: {len(Q.states)} states, {len(Q.stop)} final')\nprint(f'R: {len(R.states)} states, {len(R.stop)} final')\n\n# Enumerate: find the most likely mixed-case inputs that lowercase to target_str\npe = prioritized_enumeration(lm_wiki.initial(), fst, target, max_steps=100000, \n                             decompose=RustDecomp)\n\nprint(f'\\nFound {len(pe.quotient_terms)} quotient, {len(pe.remainder_terms)} remainder')\nprint(f'\\nMost likely inputs that lowercase to {target_str!r}:')\nall_terms = sorted(pe.quotient_terms + pe.remainder_terms, key=lambda x: -x.weight)\nfor item in all_terms[:10]:\n    kind = 'Q' if item in pe.quotient_terms else 'R'\n    input_bytes = item.source.path_bytes()\n    print(f'  [{kind}] logp={item.weight:+.3f}  {input_bytes}')"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "Q.min().graphviz(fmt_edge=lambda i,a,j: chr(a).replace(' ', '␣') if 32 <= a <= 126 else f'\\\\x{a:02x}')"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}