{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PTB Benchmark Analysis\n",
    "\n",
    "Analysis of precover decomposition benchmark results on the PTB tokenizer FST."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Audit: Inspect Individual Automata\n",
    "\n",
    "Run decomposition at a specific prefix and visualize Q/R."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import transduction_core\n",
    "from transduction.rust_bridge import to_rust_fst\n",
    "from tqdm.auto import tqdm\n",
    "from transduction.applications.ptb import build_ptb_fst_pynini, string_to_byte_strs, SEP\n",
    "from transduction.applications.wikitext import load_wikitext, wikitext_detokenize\n",
    "from transduction.fst import FST\n",
    "from transduction.fsa import EPSILON\n",
    "from transduction.rust_bridge import RustDecomp\n",
    "from transduction.vibes import visualize_automaton\n",
    "from arsenal import timeit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fmt_sym(s):\n",
    "    \"\"\"Convert byte symbol to readable string.\"\"\"\n",
    "    if s == EPSILON:\n",
    "        return '\u03b5'\n",
    "    if s == SEP:\n",
    "        return '<SEP>'\n",
    "    assert isinstance(s, int)\n",
    "    try:\n",
    "        i = int(s)        \n",
    "        if i == 32:\n",
    "            return '\u2423'  # visible space\n",
    "        return repr(bytes([i]))[2:-1]  # e.g. 'a', '\\\\n', '\\\\x00'\n",
    "    except:\n",
    "        return str(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with timeit('build'):\n",
    "    ptb_fst = build_ptb_fst_pynini()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load first paragraph\n",
    "for item in load_wikitext(\"test\"):\n",
    "    text = item[\"text\"].strip()\n",
    "    if text and not text.startswith(\"=\"):\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the tokenizer to the raw corpus to get a complete target string.\n",
    "from arsenal import timeit\n",
    "\n",
    "with timeit('transduce'):\n",
    "    detok = wikitext_detokenize(text)\n",
    "    target_full = ptb_fst.transduce(string_to_byte_strs(detok))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Text: {detok[:80]}...\")\n",
    "print('|'.join(map(fmt_sym, target_full)))\n",
    "print(f\"Total symbols: {len(target_full)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run decomposition - change PREFIX_LEN to inspect different positions\n",
    "# Try: 20, 23, 46, 49 for non-empty remainder cases\n",
    "PREFIX_LEN = 20\n",
    "\n",
    "target = target_full[:PREFIX_LEN]\n",
    "print((colors.dark.white % '|').join(map(fmt_sym, target)))\n",
    "\n",
    "from arsenal import timeit, colors\n",
    "\n",
    "with timeit('decomp'):\n",
    "    result = RustDecomp(ptb_fst, target)\n",
    "\n",
    "with timeit('post'):\n",
    "   Q, R = result.quotient, result.remainder\n",
    "\n",
    "with timeit('decomp (minimize=True)'):\n",
    "    result_min = RustDecomp(ptb_fst, target, minimize=True)\n",
    "\n",
    "with timeit('post (minimize=True)'):\n",
    "    Q_min, R_min = result_min.quotient, result_min.remainder\n",
    "\n",
    "print(f\"\\nRaw:       Q={len(Q.states)} states, R={len(R.states)} states\")\n",
    "print(f\"Minimized: Q={len(Q_min.states)} states, R={len(R_min.states)} states\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decode target\n",
    "with timeit('decode'):\n",
    "    tokens = []\n",
    "    buf = []\n",
    "    for sym in target:\n",
    "        if sym == SEP:\n",
    "            if buf: tokens.append(bytes([int(b) for b in buf]).decode('utf-8', errors='replace'))\n",
    "            buf = []\n",
    "        elif sym != EPSILON:\n",
    "            buf.append(sym)\n",
    "    if buf: tokens.append(bytes([int(b) for b in buf]).decode('utf-8', errors='replace'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Prefix {PREFIX_LEN}: {' | '.join(tokens)}\")\n",
    "print(f\"Q: {len(Q.states)} states, {len(Q.stop)} final\")\n",
    "print(f\"R: {len(R.states)} states, {len(R.stop)} final\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(Q.states), len(Q_min.states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(list(Q.arcs())), len(list(Q_min.arcs()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_automaton(Q_min.map_labels(lambda x: bytes([x])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_automaton(R_min.map_labels(lambda x: bytes([x])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = 0\n",
    "for x in Q_min.language(tuple=True):\n",
    "    t += 1\n",
    "    if t > 20: break\n",
    "    print(''.join(fmt_sym(y) for y in x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = 0\n",
    "for x in R_min.language(tuple=True):\n",
    "    t += 1\n",
    "    if t > 20: break\n",
    "    print(''.join(fmt_sym(y) for y in x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q/R Size and Timing vs. Prefix Length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cache the Rust FST conversion (avoids repeated Python\u2192Rust conversion)\nrust_fst_cached, sym_map_cached, _ = to_rust_fst(ptb_fst)\n\n# Pre-build all decomposers\ndirty_decomp = transduction_core.RustDirtyStateDecomp(rust_fst_cached)\n\n# Collect Q/R sizes and timing as a function of prefix length (all algorithms).\n# NOTE: We use wall-clock time (time.perf_counter) rather than stats.total_ms\n# because stats.total_ms excludes the ip_universal_states precomputation that\n# rust_decompose does on every call but the other variants cache.\ndata = {\n    'pos': [],\n    # rust_decompose (batch) \u2014 no caching\n    'batch_ms': [],\n    'dfa_states': [], 'Q_min': [], 'R_min': [],\n    # RustDirtyStateDecomp (persists DFA structure, skips clean states)\n    'dirty_ms': [],\n    'dirty_compute_arcs_calls': [],  # states fully expanded (dirty/border/new)\n    'dirty_intern_calls': [],        # arcs created (one intern per arc)\n    'dirty_dfa_states': [],          # total arena size (grows monotonically)\n    'dirty_eps_hits': [], 'dirty_eps_misses': [],\n}\n\nfor pos in tqdm(range(1, len(target_full) + 1), desc=\"prefix sweep\"):\n    target_u32 = [sym_map_cached(y) for y in target_full[:pos]]\n\n    # Batch decomposition (no caching); perf_counter returns seconds, *1000 \u2192 ms\n    t0 = time.perf_counter()\n    d = transduction_core.rust_decompose(rust_fst_cached, target_u32, minimize=False)\n    data['batch_ms'].append((time.perf_counter() - t0) * 1000)\n\n    # Dirty-state (persists DFA structure, only re-expands dirty/border states)\n    t0 = time.perf_counter()\n    dd = dirty_decomp.decompose(target_u32, False)\n    data['dirty_ms'].append((time.perf_counter() - t0) * 1000)\n\n    data['pos'].append(pos)\n    data['dfa_states'].append(d.stats.dfa_states)\n    data['Q_min'].append(d.quotient.num_states())\n    data['R_min'].append(d.remainder.num_states())\n    data['dirty_compute_arcs_calls'].append(dd.stats.compute_arcs_calls)\n    data['dirty_intern_calls'].append(dd.stats.intern_calls)\n    data['dirty_dfa_states'].append(dd.stats.dfa_states)\n    data['dirty_eps_hits'].append(dd.stats.eps_cache_hits)\n    data['dirty_eps_misses'].append(dd.stats.eps_cache_misses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\nimport matplotlib.pyplot as plt\n\nfig, (ax1, ax2) = plt.subplots(2, 1, figsize=(14, 9), sharex=True)\n\n# Top panel: automaton sizes\nax1.plot(data[\"pos\"], data[\"dfa_states\"], label=\"DFA states (raw)\", alpha=0.7)\nax1.plot(data[\"pos\"], data[\"Q_min\"], label=\"Q (minimized)\", alpha=0.7, linestyle=\"--\")\nax1.plot(data[\"pos\"], data[\"R_min\"], label=\"R (minimized)\", alpha=0.7, linestyle=\"--\")\nax1.set_ylabel(\"Number of states\")\nax1.set_title(\"Automaton Size vs. Prefix Length\")\nax1.legend()\nax1.grid(True, alpha=0.3)\n\n# Bottom panel: wall-clock timing (log scale)\nfor key, label, color in [\n    ('batch_ms', 'batch (no caching)', 'C0'),\n    ('dirty_ms', 'dirty-state (incr BFS + univ$)', 'C3'),\n]:\n    arr = np.array(data[key])\n    ax2.scatter(data[\"pos\"], arr, alpha=0.2, s=1, color=color)\n    # Running average\n    window = 20\n    if len(arr) >= window:\n        avg = np.convolve(arr, np.ones(window)/window, mode='valid')\n        ax2.plot(data[\"pos\"][window-1:], avg, label=label, linewidth=2, color=color, alpha=0.8)\n\nax2.set_xlabel(\"Prefix length\")\nax2.set_ylabel(\"Time (ms)\")\nax2.set_yscale(\"log\")\nax2.set_title(\"Wall-Clock Timing: Batch vs. Dirty-State\")\nax2.legend()\nax2.grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_symbols = len(target_full)\nbatch_total = sum(data['batch_ms'])\ndirty_total = sum(data['dirty_ms'])\n\nprint(f\"Total symbols: {total_symbols}\")\nprint(f\"{'Variant':<35s} {'Total ms':>10s} {'sym/sec':>10s} {'Speedup':>10s}\")\nprint(\"-\" * 70)\nfor name, total in [\n    ('batch (no caching)', batch_total),\n    ('dirty-state (incr BFS + univ$)', dirty_total),\n]:\n    throughput = total_symbols / (total / 1000)\n    speedup = batch_total / total\n    print(f\"{name:<35s} {total:>10.0f} {throughput:>10.0f} {speedup:>10.2f}x\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dirty-State Analysis\n",
    "\n",
    "The dirty-state variant persists the entire DFA structure (arena + per-state arcs + state classification) across calls. On each prefix extension, it:\n",
    "1. Marks **dirty** states (NFA set contains elements at `buf_pos >= frontier`)\n",
    "2. Marks **border** states (clean states with arcs to dirty states)\n",
    "3. Re-expands only dirty + border states; **clean states copy cached arcs**\n",
    "\n",
    "Additionally, it caches **universality results at the FST-state level**. For pure-frontier DFA states (all NFA elements at `buf_pos == target_len`), the universality sub-BFS only explores states at `buf_pos == target_len`, making the result purely FST-topology-dependent and target-independent. This cache never needs eviction and eliminates ~98% of the original runtime (the universality sub-BFS)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dirty_arr = np.array(data['dirty_ms'])\nbatch_arr = np.array(data['batch_ms'])\ndirty_arcs = np.array(data['dirty_compute_arcs_calls'])\ndfa_states = np.array(data['dfa_states'])\n\nfig, axes = plt.subplots(2, 2, figsize=(14, 10))\n\n# Top-left: states re-expanded vs total DFA states\nax = axes[0, 0]\nax.plot(data['pos'], dfa_states, label='DFA states (batch)', alpha=0.7)\nax.plot(data['pos'], dirty_arcs, label='States re-expanded (dirty)', alpha=0.7)\nax.set_xlabel('Prefix length')\nax.set_ylabel('Count')\nax.set_title('States Re-Expanded vs. Total DFA States')\nax.legend()\nax.grid(True, alpha=0.3)\n\n# Top-right: fraction of states re-expanded\nreexpand_frac = dirty_arcs / np.maximum(dfa_states, 1) * 100\nax = axes[0, 1]\nax.plot(data['pos'], reexpand_frac, alpha=0.7, linewidth=1)\nwindow = 20\nif len(reexpand_frac) >= window:\n    avg = np.convolve(reexpand_frac, np.ones(window)/window, mode='valid')\n    ax.plot(data['pos'][window-1:], avg, color='red', linewidth=2,\n            label=f'running avg (w={window}), mean={reexpand_frac[1:].mean():.1f}%')\nax.set_xlabel('Prefix length')\nax.set_ylabel('% of DFA states re-expanded')\nax.set_title('Fraction of DFA States Re-Expanded')\nax.legend()\nax.grid(True, alpha=0.3)\n\n# Bottom-left: all 3 variants timing (log scale to show dirty-state separation)\nax = axes[1, 0]\nfor arr, label, color in [\n    (batch_arr, 'batch', 'C0'),\n    (dirty_arr, 'dirty-state', 'C3'),\n]:\n    ax.scatter(data['pos'], arr, alpha=0.2, s=1, color=color)\n    if len(arr) >= window:\n        avg = np.convolve(arr, np.ones(window)/window, mode='valid')\n        ax.plot(data['pos'][window-1:], avg, label=label, linewidth=2, color=color, alpha=0.8)\nax.set_xlabel('Prefix length')\nax.set_ylabel('Time (ms)')\nax.set_yscale('log')\nax.set_title('Per-Position Timing (log scale)')\nax.legend()\nax.grid(True, alpha=0.3)\n\n# Bottom-right: per-position speedup dirty vs batch\ndirty_over_batch = batch_arr / np.maximum(dirty_arr, 0.01)\nax = axes[1, 1]\nax.scatter(data['pos'], dirty_over_batch, alpha=0.3, s=2)\nif len(dirty_over_batch) >= window:\n    avg = np.convolve(dirty_over_batch, np.ones(window)/window, mode='valid')\n    ax.plot(data['pos'][window-1:], avg, color='red', linewidth=2,\n            label=f'running avg (w={window})')\nax.axhline(y=1.0, color='gray', linestyle='--', alpha=0.5, label='break-even')\nax.set_xlabel('Prefix length')\nax.set_ylabel('Speedup (batch / dirty)')\nax.set_title('Per-Position Speedup: Dirty-State over Batch')\nax.legend()\nax.grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\n# Summary stats\nprint(f\"Mean states re-expanded per position: {dirty_arcs[1:].mean():.1f} / {dfa_states[1:].mean():.1f} \"\n      f\"({reexpand_frac[1:].mean():.1f}%)\")\nprint(f\"Mean dirty/batch speedup: {dirty_over_batch[1:].mean():.1f}x\")\nprint(f\"Median dirty/batch speedup: {np.median(dirty_over_batch[1:]):.1f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Direct Correlation: Time vs. Arc Churn\n",
    "\n",
    "The key claim for incremental decomposition is that per-step cost scales with the\n",
    "**change** in the DFA graph, not the **total** DFA size.\n",
    "\n",
    "We measure change as `intern_calls` \u2014 the number of arcs created per step (one\n",
    "`arena.intern()` call per arc). Each re-expanded state first has its old arcs\n",
    "removed, then new arcs are created via BFS, so `intern_calls` directly measures\n",
    "the arc-level work done.\n",
    "\n",
    "- **Left panel**: Scatter of per-position wall-clock time vs. arcs created\n",
    "  (`intern_calls`). A strong linear fit confirms cost \u221d arc churn.\n",
    "- **Right panel**: Scatter of per-position wall-clock time vs. total DFA arena size\n",
    "  (`dfa_states`). No correlation confirms cost is independent of total size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dirty_arr = np.array(data['dirty_ms'])\n",
    "dirty_arcs_created = np.array(data['dirty_intern_calls'])\n",
    "dirty_dfa = np.array(data['dirty_dfa_states'])\n",
    "\n",
    "# Skip step 1 (cold start) for steady-state analysis\n",
    "ss = slice(1, None)\n",
    "t_us = dirty_arr[ss] * 1000  # ms \u2192 \u03bcs\n",
    "change = dirty_arcs_created[ss]\n",
    "total = dirty_dfa[ss]\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(13, 5.5))\n",
    "\n",
    "# --- Left: time vs arcs created (should correlate) ---\n",
    "ax1.scatter(change, t_us, alpha=0.25, s=8, color='tab:blue', edgecolors='none')\n",
    "\n",
    "# Linear fit through origin: t \u2248 slope * change\n",
    "slope = np.dot(change.astype(float), t_us) / np.dot(change.astype(float), change.astype(float))\n",
    "xs = np.linspace(0, change.max() * 1.05, 100)\n",
    "ax1.plot(xs, slope * xs, 'r--', linewidth=1.5, label=f'fit: {slope:.1f} \u03bcs/arc')\n",
    "\n",
    "ss_res = np.sum((t_us - slope * change) ** 2)\n",
    "ss_tot = np.sum((t_us - t_us.mean()) ** 2)\n",
    "r2 = 1 - ss_res / ss_tot\n",
    "\n",
    "ax1.set_xlabel('arcs created (intern_calls)', fontsize=11)\n",
    "ax1.set_ylabel('time per step (\u03bcs)', fontsize=11)\n",
    "ax1.set_title(f'Time vs. Arcs Created (R\u00b2 = {r2:.2f})', fontsize=12)\n",
    "ax1.legend(fontsize=9)\n",
    "ax1.set_xlim(left=0)\n",
    "ax1.set_ylim(bottom=0)\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# --- Right: time vs total DFA size (should NOT correlate) ---\n",
    "ax2.scatter(total, t_us, alpha=0.25, s=8, color='tab:orange', edgecolors='none')\n",
    "\n",
    "r_total = np.corrcoef(total.astype(float), t_us)[0, 1]\n",
    "\n",
    "ax2.set_xlabel('total DFA arena size (dfa_states)', fontsize=11)\n",
    "ax2.set_ylabel('time per step (\u03bcs)', fontsize=11)\n",
    "ax2.set_title(f'Time vs. Total DFA Size (r = {r_total:.2f})', fontsize=12)\n",
    "ax2.set_ylim(bottom=0)\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# Add annotation\n",
    "ax2.annotate(f'DFA grows {total.min()}\u2192{total.max()} states\\n'\n",
    "             f'but time stays {np.median(t_us):.0f} \u03bcs (median)',\n",
    "             xy=(0.95, 0.95), xycoords='axes fraction',\n",
    "             ha='right', va='top', fontsize=9,\n",
    "             bbox=dict(boxstyle='round,pad=0.3', facecolor='wheat', alpha=0.5))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Time vs arcs created:    R\u00b2 = {r2:.3f}  (slope = {slope:.1f} \u03bcs/arc)\")\n",
    "print(f\"Time vs total DFA size:  r  = {r_total:.3f}\")\n",
    "print(f\"Median time: {np.median(t_us):.0f} \u03bcs, DFA range: {total.min()}\u2013{total.max()} states\")\n",
    "print(f\"Median arcs created/step: {np.median(change):.0f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions\n\n**Key finding:** The dirty-state incremental decomposition achieves **~40x speedup**\nover batch decomposition on the PTB FST.\n\nThe decisive optimization is the **FST-level universality cache**. For \"pure frontier\"\nDFA states (all NFA elements at `buf_pos == target_len`), universality depends only on\nthe FST state set, not the target string. Caching this permanently eliminates ~98% of\nthe original runtime.\n\n**Architecture:** The dirty-state variant persists: (1) the PowersetArena, (2) per-state\ncached arcs and classification (NEW/INTERIOR/QSTOP/RSTOP), (3) the UniversalityFilter,\n(4) the eps_cache, and (5) a permanent `fst_univ_cache` mapping FST state sets to\nuniversality results. On prefix extension, it marks dirty/border states for re-expansion\nwhile clean states copy cached arcs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}