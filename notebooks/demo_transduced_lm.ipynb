{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TransducedLM Demo\n",
    "\n",
    "This notebook demonstrates the `TransducedLM` — an incremental language model that computes the **pushforward** of an inner LM through an FST.\n",
    "\n",
    "Given:\n",
    "- An inner LM $P_{\\text{inner}}(\\text{source})$ over source strings\n",
    "- An FST mapping source → target\n",
    "\n",
    "TransducedLM computes $P(y \\mid \\text{target\\_so\\_far})$ by marginalizing over all source strings that produce the target prefix, using the peekaboo decomposition for incremental next-symbol computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "from transduction import examples, FST, EPSILON\n",
    "from transduction.lm.transduced import TransducedLM, logsumexp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A simple char-level n-gram LM\n",
    "\n",
    "We need an inner LM that implements the `StateLM` interface:\n",
    "- `lm.initial()` → initial state\n",
    "- `state << token` → advance by one token\n",
    "- `state.logp_next[token]` → log P(token | context)\n",
    "- `state.eos` → the EOS token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from transduction.lm.ngram import CharNgramLM"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper: display a distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def copy_fst(alphabet):\n",
    "    \"\"\"Identity/copy transducer: maps each symbol to itself.\"\"\"\n",
    "    fst = FST()\n",
    "    fst.add_I(0)\n",
    "    fst.add_F(0)\n",
    "    for x in alphabet:\n",
    "        fst.add_arc(0, x, x, 0)\n",
    "    return fst\n",
    "\n",
    "\n",
    "def show_dist(state, symbols, show_zeros=False):\n",
    "    \"\"\"Display the full next-symbol distribution including EOS.\"\"\"\n",
    "    lp = state.logp_next\n",
    "    rows = []\n",
    "    all_logps = []\n",
    "    for y in sorted(symbols):\n",
    "        v = lp[y]\n",
    "        all_logps.append(v)\n",
    "        rows.append((repr(y), v, np.exp(v) if v > -50 else 0.0))\n",
    "    eos_v = lp[state.eos]\n",
    "    all_logps.append(eos_v)\n",
    "    rows.append(('EOS', eos_v, np.exp(eos_v) if eos_v > -50 else 0.0))\n",
    "\n",
    "    total = np.exp(logsumexp(all_logps))\n",
    "    print(f\"  target = {state._peekaboo_state.target!r}\")\n",
    "    for name, logp, prob in rows:\n",
    "        if not show_zeros and prob < 1e-6:\n",
    "            continue\n",
    "        bar = '#' * int(prob * 40)\n",
    "        print(f\"  P({name:>5s}) = {prob:.4f}  {bar}\")\n",
    "    print(f\"  {'sum':>10s} = {total:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Copy FST: TransducedLM reproduces the inner LM\n",
    "\n",
    "With an identity transducer (each symbol maps to itself), the transduced distribution should match the inner LM exactly — including EOS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inner = CharNgramLM.train(\"aabbaabb\" * 10, n=2, alpha=0.5)\n",
    "fst = copy_fst([s for s in inner.alphabet if s != '<EOS>'])\n",
    "symbols = sorted(fst.B - {EPSILON})\n",
    "\n",
    "tlm = TransducedLM(inner, fst, max_steps=2000, max_beam=200)\n",
    "state = tlm.initial()\n",
    "\n",
    "print(\"From empty prefix:\")\n",
    "show_dist(state, symbols)\n",
    "\n",
    "print(\"\\nInner LM comparison:\")\n",
    "inner_state = inner.initial()\n",
    "for y in symbols:\n",
    "    print(f\"  {y!r}: inner={inner_state.logp_next[y]:.4f}  transduced={state.logp_next[y]:.4f}\")\n",
    "print(f\"  EOS: inner={inner_state.logp_next['<EOS>']:.4f}  transduced={state.logp_next[state.eos]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"After advancing by 'a':\")\n",
    "state_a = state << 'a'\n",
    "show_dist(state_a, symbols)\n",
    "\n",
    "print(\"\\nAfter advancing by 'a' then 'b':\")\n",
    "state_ab = state_a << 'b'\n",
    "show_dist(state_ab, symbols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Non-trivial FST: `examples.small()`\n",
    "\n",
    "The `small()` FST has structure:\n",
    "- State 0 (initial, final): `a→x→1`, `b→x→2`\n",
    "- State 1 (final): no outgoing arcs\n",
    "- State 2: `a→a→3`, `b→b→3`\n",
    "- State 3 (final): `a→a→3`, `b→b→3`\n",
    "\n",
    "So possible outputs are: `ε` (empty, from state 0), `x` (from source `a`, via state 1), and `x` followed by any string of `a`s and `b`s (from source `b...`, via states 2→3).\n",
    "\n",
    "Note that state 0 is final, so the empty source string produces the empty output — this gives non-trivial P(EOS) from the initial state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fst2 = examples.small()\n",
    "inner2 = CharNgramLM.train(\"aababba\" * 10, n=2, alpha=0.5)\n",
    "symbols2 = sorted(fst2.B - {EPSILON})\n",
    "\n",
    "print(f\"Input alphabet:  {sorted(fst2.A - {EPSILON})}\")\n",
    "print(f\"Output alphabet: {symbols2}\")\n",
    "print(f\"Initial: {sorted(fst2.I)}, Final: {sorted(fst2.F)}\")\n",
    "print()\n",
    "\n",
    "tlm2 = TransducedLM(inner2, fst2, max_steps=2000, max_beam=200)\n",
    "\n",
    "s = tlm2.initial()\n",
    "print(\"Empty prefix — only 'x' and EOS are reachable:\")\n",
    "show_dist(s, symbols2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_x = s << 'x'\n",
    "print(\"After 'x':\")\n",
    "print(\"  Source 'a' maps to just 'x' (state 1, final, no outgoing arcs)\")\n",
    "print(\"  Source 'b...' maps to 'x' + continuation, but the LM puts\")\n",
    "print(\"  most mass on 'a', so almost all probability goes to EOS:\")\n",
    "show_dist(s_x, symbols2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# togglecase() maps a→A, b→B, A→a, B→b (and space→space)\n",
    "# With an inner LM trained on lowercase, the output will be uppercase.\n",
    "fst_tc = examples.togglecase()\n",
    "target_alpha_tc = sorted(fst_tc.B - {EPSILON})\n",
    "source_alpha_tc = sorted(fst_tc.A - {EPSILON})\n",
    "print(f\"togglecase: {source_alpha_tc} → {target_alpha_tc}\")\n",
    "\n",
    "inner_tc = CharNgramLM.train(\"ab ba ab ba\" * 10, n=2, alpha=0.1)\n",
    "tlm_tc = TransducedLM(inner_tc, fst_tc, max_steps=2000, max_beam=200)\n",
    "\n",
    "s = tlm_tc.initial()\n",
    "print(\"\\nEmpty prefix — uppercase symbols should dominate:\")\n",
    "show_dist(s, target_alpha_tc)\n",
    "\n",
    "s = s << 'A'\n",
    "print(\"\\nAfter 'A' (inner LM saw 'a' → next likely 'b' → output 'B'):\")\n",
    "show_dist(s, target_alpha_tc)\n",
    "\n",
    "s = s << 'B'\n",
    "print(\"\\nAfter 'AB' (inner saw 'ab' → next likely ' ' → output ' '):\")\n",
    "show_dist(s, target_alpha_tc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Autoregressive decoding\n",
    "\n",
    "We can use `TransducedLM` for greedy (or sampled) autoregressive decoding, stopping when EOS is the most likely next token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# greedy_decode and sample_decode are now methods on any LM state\n# via LMState — just call state.greedy_decode() or state.sample_decode().\n#\n# For verbose step-by-step output in this demo, we use a small wrapper:\n\ndef show_greedy(state, max_len=15):\n    \"\"\"Greedy decode with per-step printing.\"\"\"\n    output = []\n    for step in range(max_len):\n        lp = state.logp_next\n        best_tok = lp.argmax()\n        best_lp = lp[best_tok]\n        eos_lp = lp[state.eos]\n        if best_tok == state.eos:\n            print(f\"  step {step}: EOS (logp={eos_lp:.3f})\")\n            break\n        output.append(best_tok)\n        print(f\"  step {step}: {best_tok!r}  (logp={best_lp:.3f}, P(EOS)={np.exp(eos_lp):.4f})\")\n        state = state << best_tok\n    return output, state"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "inner3 = CharNgramLM.train(\"abababab\" * 20, n=2, alpha=0.1)\nfst3 = copy_fst(['a', 'b'])\nsymbols3 = sorted(fst3.B - {EPSILON})\n\ntlm3 = TransducedLM(inner3, fst3, max_steps=2000, max_beam=200)\n\nprint(\"Greedy decoding with copy FST (inner LM trained on 'abababab'):\")\ndecoded, final_state = show_greedy(tlm3.initial())\nprint(f\"  => {''.join(str(t) for t in decoded)!r}  (logp={final_state.logp:.4f})\")\n\n# Equivalently, without verbose output:\ntokens = tlm3.initial().greedy_decode(max_len=15)\nprint(f\"\\n  state.greedy_decode() => {tokens!r}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print(\"Sampling (5 draws) via state.sample_decode():\")\nfor i in range(5):\n    tokens = tlm3.initial().sample_decode(max_len=15)\n    print(f\"  {i}: {''.join(str(t) for t in tokens)!r}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Decoding through a non-trivial FST\n",
    "\n",
    "Decode through `togglecase()` — the inner LM is trained on lowercase text, but the output is uppercase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "inner4 = CharNgramLM.train(\"ab ba ab ba ab\" * 10, n=2, alpha=0.1)\nfst4 = examples.togglecase()\nsymbols4 = sorted(fst4.B - {EPSILON})\n\ntlm4 = TransducedLM(inner4, fst4, max_steps=2000, max_beam=200)\n\nprint(\"Greedy decoding through togglecase() FST:\")\ndecoded, _ = show_greedy(tlm4.initial())\nprint(f\"  => {''.join(str(t) for t in decoded)!r}\")\n\nprint(\"\\nSampling (5 draws) via state.sample_decode():\")\nfor i in range(5):\n    tokens = tlm4.initial().sample_decode(max_len=15)\n    print(f\"  {i}: {''.join(str(t) for t in tokens)!r}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Normalization check\n",
    "\n",
    "Verify that at each step, the distribution over symbols + EOS sums to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inner5 = CharNgramLM.train(\"aabbaabb\" * 10, n=2, alpha=0.5)\n",
    "fst5 = copy_fst([s for s in inner5.alphabet if s != '<EOS>'])\n",
    "symbols5 = sorted(fst5.B - {EPSILON})\n",
    "\n",
    "tlm5 = TransducedLM(inner5, fst5, max_steps=2000, max_beam=200)\n",
    "state = tlm5.initial()\n",
    "\n",
    "for target_sym in ['a', 'b', 'a', 'b']:\n",
    "    lp = state.logp_next\n",
    "    all_logps = [lp[y] for y in symbols5] + [lp[state.eos]]\n",
    "    total = np.exp(logsumexp(all_logps))\n",
    "    print(f\"  target={state._peekaboo_state.target!r:>6s}  \"\n",
    "          f\"sum={total:.10f}  \"\n",
    "          f\"P(EOS)={np.exp(lp[state.eos]):.6f}\")\n",
    "    state = state << target_sym\n",
    "\n",
    "# Final state\n",
    "lp = state.logp_next\n",
    "all_logps = [lp[y] for y in symbols5] + [lp[state.eos]]\n",
    "total = np.exp(logsumexp(all_logps))\n",
    "print(f\"  target={state._peekaboo_state.target!r:>6s}  \"\n",
    "      f\"sum={total:.10f}  \"\n",
    "      f\"P(EOS)={np.exp(lp[state.eos]):.6f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}